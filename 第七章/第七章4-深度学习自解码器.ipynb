{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15a976e-c3a1-46ab-990c-d5dddd45d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 21:45:23.792334: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Volatility     Beta1     Beta2      Cor1      Cor2  NonSysRisk1  \\\n",
      "0        0.583365  0.716008  0.726709  0.184198  0.188873     0.012349   \n",
      "1        0.383995  0.913849  0.923003  0.409555  0.416662     0.025676   \n",
      "2        0.423533  0.806881  0.831737  0.278495  0.292493     0.014550   \n",
      "3        0.518040  0.943050  0.929599  0.254375  0.254799     0.018665   \n",
      "4        0.550201  1.066480  1.068627  0.284123  0.290532     0.025388   \n",
      "...           ...       ...       ...       ...       ...          ...   \n",
      "78552    0.555577  0.944254  0.961975  0.247322  0.256579     0.019359   \n",
      "78553    0.471594  1.882598  1.846909  0.568810  0.567869     0.074968   \n",
      "78554    0.649889  1.180658  1.213428  0.317196  0.328823     0.042696   \n",
      "78555    0.632866  1.286845  1.315512  0.349781  0.361143     0.049497   \n",
      "78556    0.645599  1.472637  1.493594  0.335231  0.346022     0.047210   \n",
      "\n",
      "       NonSysRisk2      Rsq1      Rsq2     ARsq1  ...  \\\n",
      "0         0.012984  0.033929  0.035673  0.030033  ...   \n",
      "1         0.026575  0.167735  0.173607  0.164379  ...   \n",
      "2         0.016050  0.077559  0.085552  0.073840  ...   \n",
      "3         0.018727  0.064707  0.064923  0.060935  ...   \n",
      "4         0.026546  0.080726  0.084409  0.077019  ...   \n",
      "...            ...       ...       ...       ...  ...   \n",
      "78552     0.020836  0.061168  0.065833  0.057383  ...   \n",
      "78553     0.074720  0.323544  0.322475  0.320817  ...   \n",
      "78554     0.045884  0.100613  0.108125  0.096987  ...   \n",
      "78555     0.052765  0.122346  0.130424  0.118808  ...   \n",
      "78556     0.050299  0.112380  0.119731  0.108801  ...   \n",
      "\n",
      "       EquityGrowth RevenueGrowth  EquityGrowth OperatingNCFGrowth  \\\n",
      "0                        0.011834                        -0.000247   \n",
      "1                        0.006640                        -0.027520   \n",
      "2                       -0.001676                        -0.014111   \n",
      "3                        0.008637                         0.046943   \n",
      "4                        0.013462                         0.074142   \n",
      "...                           ...                              ...   \n",
      "78552                    0.763005                         2.402207   \n",
      "78553                    0.055161                         0.096932   \n",
      "78554                   24.044883                         0.217277   \n",
      "78555                    0.258489                        -0.917388   \n",
      "78556                    0.040939                         0.071035   \n",
      "\n",
      "       EquityGrowth OGS  EquityGrowth VCG  RevenueGrowth OperatingNCFGrowth  \\\n",
      "0              0.006614          0.001328                         -0.001095   \n",
      "1              0.013553          0.001732                         -0.010093   \n",
      "2              0.010485         -0.010064                          0.000691   \n",
      "3              0.013270          0.005609                          0.167349   \n",
      "4              0.046026          0.041144                          0.222097   \n",
      "...                 ...               ...                               ...   \n",
      "78552          1.318186          1.245228                          4.093123   \n",
      "78553          0.064398          0.057856                          0.134478   \n",
      "78554         32.875337         32.757076                          2.009641   \n",
      "78555          0.009852         -0.006237                         -0.724709   \n",
      "78556          0.073448          0.068323                          0.031394   \n",
      "\n",
      "       RevenueGrowth OGS  RevenueGrowth VCG  OperatingNCFGrowth OGS  \\\n",
      "0               0.029273           0.005879               -0.000612   \n",
      "1               0.004971           0.000635               -0.020600   \n",
      "2              -0.000514           0.000493               -0.004326   \n",
      "3               0.047309           0.019998                0.257121   \n",
      "4               0.137873           0.123250                0.759313   \n",
      "...                  ...                ...                     ...   \n",
      "78552           2.246058           2.121745                7.071373   \n",
      "78553           0.089342           0.080266                0.156997   \n",
      "78554         304.071218         302.977398                2.747679   \n",
      "78555           0.007783          -0.004927               -0.027621   \n",
      "78556           0.032461           0.030196                0.056324   \n",
      "\n",
      "       OperatingNCFGrowth VCG     OGS VCG  \n",
      "0                   -0.000123    0.003286  \n",
      "1                   -0.002633    0.001297  \n",
      "2                    0.004152   -0.003085  \n",
      "3                    0.108686    0.030725  \n",
      "4                    0.678781    0.421372  \n",
      "...                       ...         ...  \n",
      "78552                6.679992    3.665575  \n",
      "78553                0.141047    0.093707  \n",
      "78554                2.737795  414.245483  \n",
      "78555                0.017486   -0.000188  \n",
      "78556                0.052394    0.054174  \n",
      "\n",
      "[78557 rows x 630 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 数据整理\n",
    "data = pd.read_csv('/Users/xiaoquanliu/Desktop/Book_DataCode/第七章/DL_Data2.csv')\n",
    "data1=data.dropna()\n",
    "\n",
    "features = data1.iloc[:, :-1] \n",
    "target = data1.iloc[:, -1]\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "# 创建一个 PolynomialFeatures 对象，设置 degree=2 来生成两两交互的特征\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "# 使用 PolynomialFeatures 对象转换你的数据\n",
    "features_poly = poly.fit_transform(features)\n",
    "\n",
    "# 将交互特征转换为 DataFrame，并添加列名\n",
    "features_poly_df = pd.DataFrame(features_poly, columns=poly.get_feature_names_out(features.columns))\n",
    "\n",
    "# 打印结果\n",
    "print(features_poly_df)\n",
    "\n",
    "# features_poly_df 包含了原始特征和它们的两两交互项\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_poly_df)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "input_dim = features_train.shape[1]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806619c5-a79d-4fb3-83bb-834edfa111c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 21:45:31.590287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 1s 5ms/step - loss: 0.1637 - val_loss: 0.1603\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.1561 - val_loss: 0.1509\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.1439 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.1266 - val_loss: 0.1166\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.1062 - val_loss: 0.0957\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0858 - val_loss: 0.0761\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0675 - val_loss: 0.0593\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0524 - val_loss: 0.0460\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0406 - val_loss: 0.0356\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0316 - val_loss: 0.0278\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0246 - val_loss: 0.0218\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0194 - val_loss: 0.0173\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0157 - val_loss: 0.0142\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0130 - val_loss: 0.0119\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0090\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0081\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0074\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 24/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 25/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 26/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 27/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 28/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 29/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 30/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 31/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 32/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 33/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 34/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 35/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 36/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 37/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 38/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 39/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 40/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 41/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 42/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 43/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 44/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 45/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 46/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 47/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 48/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 49/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 50/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 51/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 52/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 53/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 54/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 55/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 56/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 57/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 58/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 59/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 60/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 61/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 62/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 63/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 64/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 65/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 66/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 67/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 68/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 69/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 70/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 71/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 72/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 73/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 74/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 75/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 76/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 77/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 78/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 79/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 80/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 81/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 82/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 83/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 84/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 85/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 86/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 87/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 88/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 89/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 90/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 91/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 92/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 93/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 94/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 95/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 96/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 97/100\n",
      "123/123 [==============================] - 1s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 98/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 99/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 100/100\n",
      "123/123 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "1964/1964 [==============================] - 1s 397us/step\n",
      "491/491 [==============================] - 0s 434us/step\n",
      "1964/1964 [==============================] - 1s 399us/step\n",
      "Mean Squared Error on test set:  0.00036902812574674107\n",
      "Coefficient for encoded feature 1: -0.004148242063820362\n",
      "Coefficient for encoded feature 2: -0.005050643812865019\n",
      "Coefficient for encoded feature 3: 0.053775474429130554\n",
      "Coefficient for encoded feature 4: -0.5449056029319763\n",
      "Coefficient for encoded feature 5: -0.013774795457720757\n",
      "Coefficient for encoded feature 6: 0.07581759244203568\n",
      "Coefficient for encoded feature 7: 5.069654434919357e-06\n",
      "Coefficient for encoded feature 8: -0.0018262690864503384\n",
      "Coefficient for encoded feature 9: -0.0509047731757164\n",
      "Coefficient for encoded feature 10: 0.03240454941987991\n",
      "Coefficient for encoded feature 11: -1.0749325156211853e-05\n",
      "Coefficient for encoded feature 12: 0.06071801111102104\n",
      "Coefficient for encoded feature 13: -0.0652681216597557\n",
      "Coefficient for encoded feature 14: 0.03532727062702179\n",
      "Coefficient for encoded feature 15: -0.006075626239180565\n",
      "Coefficient for encoded feature 16: 0.014936661347746849\n",
      "Coefficient for encoded feature 17: 0.008249814622104168\n",
      "Coefficient for encoded feature 18: 0.021655287593603134\n",
      "Coefficient for encoded feature 19: 0.06344833225011826\n",
      "Coefficient for encoded feature 20: -0.016272643581032753\n",
      "Coefficient for encoded feature 21: 0.01027444377541542\n",
      "Coefficient for encoded feature 22: -0.013087579049170017\n",
      "Coefficient for encoded feature 23: -0.01580888219177723\n",
      "Coefficient for encoded feature 24: -0.0628867819905281\n",
      "Coefficient for encoded feature 25: 0.01811155676841736\n",
      "Coefficient for encoded feature 26: -0.06983904540538788\n",
      "Coefficient for encoded feature 27: -0.011327628046274185\n",
      "Coefficient for encoded feature 28: 0.06805384904146194\n",
      "Coefficient for encoded feature 29: 0.020347144454717636\n",
      "Coefficient for encoded feature 30: -0.027416104450821877\n",
      "Coefficient for encoded feature 31: -0.009275320917367935\n",
      "Coefficient for encoded feature 32: -0.03675078600645065\n",
      "Coefficient for encoded feature 33: -0.02291250228881836\n",
      "Coefficient for encoded feature 34: -0.01543521136045456\n",
      "Coefficient for encoded feature 35: 2.7474015951156616e-07\n",
      "Coefficient for encoded feature 36: -0.005460258573293686\n",
      "Coefficient for encoded feature 37: -0.023587796837091446\n",
      "Coefficient for encoded feature 38: -0.02370992675423622\n",
      "Coefficient for encoded feature 39: 4.686415195465088e-06\n",
      "Coefficient for encoded feature 40: 0.008977316319942474\n",
      "Coefficient for encoded feature 41: 3.0212104320526123e-06\n",
      "Coefficient for encoded feature 42: 0.6880059242248535\n",
      "Coefficient for encoded feature 43: 0.042986445128917694\n",
      "Coefficient for encoded feature 44: 0.047791458666324615\n",
      "Coefficient for encoded feature 45: -1.087784767150879e-06\n",
      "Coefficient for encoded feature 46: 0.010352622717618942\n",
      "Coefficient for encoded feature 47: 0.029544999822974205\n",
      "Coefficient for encoded feature 48: 0.00045016780495643616\n",
      "Coefficient for encoded feature 49: -0.02201695367693901\n",
      "Coefficient for encoded feature 50: -0.02519773691892624\n",
      "Coefficient for encoded feature 51: -0.011760398745536804\n",
      "Coefficient for encoded feature 52: 0.033708520233631134\n",
      "Coefficient for encoded feature 53: -0.0009556934237480164\n",
      "Coefficient for encoded feature 54: 0.031185023486614227\n",
      "Coefficient for encoded feature 55: 0.010467573069036007\n",
      "Coefficient for encoded feature 56: 0.03708900883793831\n",
      "Coefficient for encoded feature 57: 0.0771443322300911\n",
      "Coefficient for encoded feature 58: -0.06669208407402039\n",
      "Coefficient for encoded feature 59: 0.0021275803446769714\n",
      "Coefficient for encoded feature 60: 2.462416887283325e-06\n",
      "Coefficient for encoded feature 61: -0.04282114654779434\n",
      "Coefficient for encoded feature 62: -0.01798234134912491\n",
      "Coefficient for encoded feature 63: 0.06022775545716286\n",
      "Coefficient for encoded feature 64: -0.01517079770565033\n",
      "Coefficient for encoded feature 65: 5.066394805908203e-07\n",
      "Coefficient for encoded feature 66: -1.0503544807434082\n",
      "Coefficient for encoded feature 67: -0.03407642990350723\n",
      "Coefficient for encoded feature 68: 0.048866190016269684\n",
      "Coefficient for encoded feature 69: -1.519918441772461e-06\n",
      "Coefficient for encoded feature 70: -0.1439313292503357\n",
      "Coefficient for encoded feature 71: -0.005035612732172012\n",
      "Coefficient for encoded feature 72: 0.016310803592205048\n",
      "Coefficient for encoded feature 73: 0.05344972014427185\n",
      "Coefficient for encoded feature 74: -0.020036257803440094\n",
      "Coefficient for encoded feature 75: -0.004555098712444305\n",
      "Coefficient for encoded feature 76: -0.06438691914081573\n",
      "Coefficient for encoded feature 77: 0.004418659955263138\n",
      "Coefficient for encoded feature 78: 0.039719708263874054\n",
      "Coefficient for encoded feature 79: 0.019894331693649292\n",
      "Coefficient for encoded feature 80: 0.05888347327709198\n",
      "Coefficient for encoded feature 81: 0.005305163562297821\n",
      "Coefficient for encoded feature 82: 0.11231016367673874\n",
      "Coefficient for encoded feature 83: -0.017478808760643005\n",
      "Coefficient for encoded feature 84: -0.0053930580615997314\n",
      "Coefficient for encoded feature 85: 0.02097451686859131\n",
      "Coefficient for encoded feature 86: 0.035042405128479004\n",
      "Coefficient for encoded feature 87: 0.023170068860054016\n",
      "Coefficient for encoded feature 88: 2.5704503059387207e-07\n",
      "Coefficient for encoded feature 89: 0.04665371775627136\n",
      "Coefficient for encoded feature 90: 0.029433850198984146\n",
      "Coefficient for encoded feature 91: -0.0384373664855957\n",
      "Coefficient for encoded feature 92: 1.3113021850585938e-06\n",
      "Coefficient for encoded feature 93: 0.02613925188779831\n",
      "Coefficient for encoded feature 94: -0.0031885653734207153\n",
      "Coefficient for encoded feature 95: -0.001970222219824791\n",
      "Coefficient for encoded feature 96: -0.036440663039684296\n",
      "Coefficient for encoded feature 97: 0.01930510625243187\n",
      "Coefficient for encoded feature 98: -0.030566424131393433\n",
      "Coefficient for encoded feature 99: 0.014938842505216599\n",
      "Coefficient for encoded feature 100: -5.885958671569824e-07\n",
      "Coefficient for encoded feature 101: -0.06721439212560654\n",
      "Coefficient for encoded feature 102: -0.03767063468694687\n",
      "Coefficient for encoded feature 103: -0.05524632707238197\n",
      "Coefficient for encoded feature 104: -0.09427479654550552\n",
      "Coefficient for encoded feature 105: 0.015928730368614197\n",
      "Coefficient for encoded feature 106: 0.036329373717308044\n",
      "Coefficient for encoded feature 107: 0.021412182599306107\n",
      "Coefficient for encoded feature 108: -0.03469158709049225\n",
      "Coefficient for encoded feature 109: -0.11438131332397461\n",
      "Coefficient for encoded feature 110: -0.03386373817920685\n",
      "Coefficient for encoded feature 111: 0.028693659231066704\n",
      "Coefficient for encoded feature 112: -0.02677558735013008\n",
      "Coefficient for encoded feature 113: 0.015071563422679901\n",
      "Coefficient for encoded feature 114: -0.017582383006811142\n",
      "Coefficient for encoded feature 115: -0.004291057586669922\n",
      "Coefficient for encoded feature 116: -0.010637342929840088\n",
      "Coefficient for encoded feature 117: -0.007071897387504578\n",
      "Coefficient for encoded feature 118: 0.038904447108507156\n",
      "Coefficient for encoded feature 119: -0.0030266400426626205\n",
      "Coefficient for encoded feature 120: 0.0005907490849494934\n",
      "Intercept: 0.01242499053478241\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Return   R-squared:                       0.224\n",
      "Model:                            OLS   Adj. R-squared:                  0.223\n",
      "Method:                 Least Squares   F-statistic:                     167.7\n",
      "Date:                Sun, 15 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        21:46:28   Log-Likelihood:             1.5799e+05\n",
      "No. Observations:               62845   AIC:                        -3.158e+05\n",
      "Df Residuals:                   62736   BIC:                        -3.148e+05\n",
      "Df Model:                         108                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0124      0.010      1.212      0.226      -0.008       0.033\n",
      "x1            -0.0041      0.008     -0.493      0.622      -0.021       0.012\n",
      "x2            -0.0050      0.004     -1.305      0.192      -0.013       0.003\n",
      "x3             0.0538      0.006      8.824      0.000       0.042       0.066\n",
      "x4            -0.5449      0.525     -1.039      0.299      -1.573       0.483\n",
      "x5            -0.0138      0.006     -2.183      0.029      -0.026      -0.001\n",
      "x6             0.0758      0.007     11.407      0.000       0.063       0.089\n",
      "x7          4.934e-13   4.59e-11      0.011      0.991   -8.94e-11    9.04e-11\n",
      "x8            -0.0018      0.007     -0.263      0.793      -0.015       0.012\n",
      "x9            -0.0509      0.007     -7.772      0.000      -0.064      -0.038\n",
      "x10            0.0324      0.007      4.724      0.000       0.019       0.046\n",
      "x11        -2.965e-14   4.02e-12     -0.007      0.994   -7.92e-12    7.86e-12\n",
      "x12            0.0607      0.006      9.564      0.000       0.048       0.073\n",
      "x13           -0.0653      0.008     -8.350      0.000      -0.081      -0.050\n",
      "x14            0.0353      0.007      5.314      0.000       0.022       0.048\n",
      "x15           -0.0061      0.005     -1.206      0.228      -0.016       0.004\n",
      "x16            0.0149      0.008      1.841      0.066      -0.001       0.031\n",
      "x17            0.0082      0.007      1.174      0.240      -0.006       0.022\n",
      "x18            0.0217      0.008      2.820      0.005       0.007       0.037\n",
      "x19            0.0634      0.008      7.839      0.000       0.048       0.079\n",
      "x20           -0.0163      0.006     -2.669      0.008      -0.028      -0.004\n",
      "x21            0.0103      0.007      1.538      0.124      -0.003       0.023\n",
      "x22           -0.0131      0.007     -1.871      0.061      -0.027       0.001\n",
      "x23           -0.0158      0.007     -2.294      0.022      -0.029      -0.002\n",
      "x24           -0.0629      0.008     -7.956      0.000      -0.078      -0.047\n",
      "x25            0.0181      0.007      2.669      0.008       0.005       0.031\n",
      "x26           -0.0698      0.008     -9.301      0.000      -0.085      -0.055\n",
      "x27           -0.0113      0.007     -1.580      0.114      -0.025       0.003\n",
      "x28            0.0681      0.007      9.970      0.000       0.055       0.081\n",
      "x29            0.0203      0.008      2.610      0.009       0.005       0.036\n",
      "x30           -0.0274      0.007     -4.054      0.000      -0.041      -0.014\n",
      "x31           -0.0093      0.008     -1.175      0.240      -0.025       0.006\n",
      "x32           -0.0367      0.006     -6.304      0.000      -0.048      -0.025\n",
      "x33           -0.0229      0.007     -3.269      0.001      -0.037      -0.009\n",
      "x34           -0.0154      0.007     -2.229      0.026      -0.029      -0.002\n",
      "x35          9.71e-15   1.23e-12      0.008      0.994    -2.4e-12    2.42e-12\n",
      "x36           -0.0055      0.006     -0.874      0.382      -0.018       0.007\n",
      "x37           -0.0236      0.006     -3.981      0.000      -0.035      -0.012\n",
      "x38           -0.0237      0.008     -3.106      0.002      -0.039      -0.009\n",
      "x39        -2.487e-15   2.12e-13     -0.012      0.991   -4.19e-13    4.14e-13\n",
      "x40            0.0090      0.004      2.175      0.030       0.001       0.017\n",
      "x41        -5.856e-15   5.33e-15     -1.099      0.272   -1.63e-14    4.59e-15\n",
      "x42            0.6880      0.259      2.652      0.008       0.180       1.196\n",
      "x43            0.0430      0.008      5.628      0.000       0.028       0.058\n",
      "x44            0.0478      0.006      7.581      0.000       0.035       0.060\n",
      "x45        -5.808e-15   7.94e-13     -0.007      0.994   -1.56e-12    1.55e-12\n",
      "x46            0.0104      0.004      2.494      0.013       0.002       0.018\n",
      "x47            0.0295      0.006      4.757      0.000       0.017       0.042\n",
      "x48            0.0004      0.007      0.067      0.946      -0.013       0.014\n",
      "x49           -0.0220      0.008     -2.921      0.003      -0.037      -0.007\n",
      "x50           -0.0252      0.006     -4.103      0.000      -0.037      -0.013\n",
      "x51           -0.0118      0.007     -1.747      0.081      -0.025       0.001\n",
      "x52            0.0337      0.008      4.444      0.000       0.019       0.049\n",
      "x53           -0.0010      0.002     -0.563      0.573      -0.004       0.002\n",
      "x54            0.0312      0.007      4.194      0.000       0.017       0.046\n",
      "x55            0.0105      0.007      1.557      0.120      -0.003       0.024\n",
      "x56            0.0371      0.006      6.566      0.000       0.026       0.048\n",
      "x57            0.0771      0.008      9.806      0.000       0.062       0.093\n",
      "x58           -0.0667      0.007     -9.427      0.000      -0.081      -0.053\n",
      "x59            0.0021      0.001      1.430      0.153      -0.001       0.005\n",
      "x60        -1.489e-15   8.46e-13     -0.002      0.999   -1.66e-12    1.66e-12\n",
      "x61           -0.0428      0.008     -5.568      0.000      -0.058      -0.028\n",
      "x62           -0.0180      0.007     -2.480      0.013      -0.032      -0.004\n",
      "x63            0.0602      0.007      8.840      0.000       0.047       0.074\n",
      "x64           -0.0152      0.007     -2.212      0.027      -0.029      -0.002\n",
      "x65         1.334e-14   1.57e-12      0.008      0.993   -3.07e-12     3.1e-12\n",
      "x66           -1.0504      0.416     -2.527      0.012      -1.865      -0.236\n",
      "x67           -0.0341      0.008     -4.240      0.000      -0.050      -0.018\n",
      "x68            0.0489      0.006      7.668      0.000       0.036       0.061\n",
      "x69        -6.788e-16   3.66e-13     -0.002      0.999   -7.18e-13    7.17e-13\n",
      "x70           -0.1439      0.053     -2.718      0.007      -0.248      -0.040\n",
      "x71           -0.0050      0.004     -1.405      0.160      -0.012       0.002\n",
      "x72            0.0163      0.007      2.263      0.024       0.002       0.030\n",
      "x73            0.0534      0.006      9.581      0.000       0.043       0.064\n",
      "x74           -0.0200      0.007     -2.974      0.003      -0.033      -0.007\n",
      "x75           -0.0046      0.003     -1.634      0.102      -0.010       0.001\n",
      "x76           -0.0644      0.008     -7.905      0.000      -0.080      -0.048\n",
      "x77            0.0044      0.007      0.641      0.522      -0.009       0.018\n",
      "x78            0.0397      0.007      5.918      0.000       0.027       0.053\n",
      "x79            0.0199      0.006      3.442      0.001       0.009       0.031\n",
      "x80            0.0589      0.009      6.692      0.000       0.042       0.076\n",
      "x81            0.0053      0.007      0.738      0.460      -0.009       0.019\n",
      "x82            0.1122      1.235      0.091      0.928      -2.308       2.532\n",
      "x83           -0.0175      0.007     -2.445      0.014      -0.031      -0.003\n",
      "x84           -0.0054      0.006     -0.866      0.386      -0.018       0.007\n",
      "x85            0.0210      0.006      3.391      0.001       0.009       0.033\n",
      "x86            0.0350      0.006      5.439      0.000       0.022       0.048\n",
      "x87            0.0236      2.859      0.008      0.993      -5.580       5.628\n",
      "x88         1.926e-16   6.13e-14      0.003      0.997    -1.2e-13     1.2e-13\n",
      "x89            0.0467      0.004     11.819      0.000       0.039       0.054\n",
      "x90            0.0294      0.008      3.532      0.000       0.013       0.046\n",
      "x91           -0.0384      0.006     -5.920      0.000      -0.051      -0.026\n",
      "x92         1.355e-15    2.4e-13      0.006      0.995   -4.68e-13    4.71e-13\n",
      "x93            0.0261      0.007      3.787      0.000       0.013       0.040\n",
      "x94           -0.0032      0.008     -0.412      0.681      -0.018       0.012\n",
      "x95           -0.0020      0.005     -0.421      0.674      -0.011       0.007\n",
      "x96           -0.0364      0.008     -4.855      0.000      -0.051      -0.022\n",
      "x97            0.0193      0.007      2.736      0.006       0.005       0.033\n",
      "x98           -0.0306      0.007     -4.203      0.000      -0.045      -0.016\n",
      "x99            0.0149      0.008      1.852      0.064      -0.001       0.031\n",
      "x100       -7.663e-16   7.45e-15     -0.103      0.918   -1.54e-14    1.38e-14\n",
      "x101          -0.0672      0.007     -9.941      0.000      -0.080      -0.054\n",
      "x102          -0.0377      0.007     -5.242      0.000      -0.052      -0.024\n",
      "x103          -0.0552      0.005    -12.112      0.000      -0.064      -0.046\n",
      "x104          -0.0943      0.008    -11.831      0.000      -0.110      -0.079\n",
      "x105           0.0159      0.008      2.091      0.037       0.001       0.031\n",
      "x106           0.0363      0.007      5.416      0.000       0.023       0.049\n",
      "x107           0.0214      0.005      4.241      0.000       0.012       0.031\n",
      "x108          -0.0347      0.006     -6.140      0.000      -0.046      -0.024\n",
      "x109          -0.1144      0.008    -14.450      0.000      -0.130      -0.099\n",
      "x110          -0.0339      0.009     -3.953      0.000      -0.051      -0.017\n",
      "x111           0.0287      0.007      4.288      0.000       0.016       0.042\n",
      "x112          -0.0268      0.008     -3.385      0.001      -0.042      -0.011\n",
      "x113           0.0151      0.008      1.939      0.053      -0.000       0.030\n",
      "x114          -0.0176      0.007     -2.700      0.007      -0.030      -0.005\n",
      "x115          -0.0043      0.008     -0.560      0.575      -0.019       0.011\n",
      "x116          -0.0106      0.008     -1.407      0.159      -0.025       0.004\n",
      "x117          -0.0071      0.007     -0.962      0.336      -0.021       0.007\n",
      "x118           0.0389      0.006      6.503      0.000       0.027       0.051\n",
      "x119          -0.0030      0.007     -0.410      0.682      -0.017       0.011\n",
      "x120           0.0006      0.001      0.396      0.692      -0.002       0.004\n",
      "==============================================================================\n",
      "Omnibus:                    22706.400   Durbin-Watson:                   1.987\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           123137.057\n",
      "Skew:                           1.657   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.004   Cond. No.                     1.00e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.03e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#设置隐藏因子的数量\n",
    "encoding_dim = 120\n",
    "hidden_dim=315\n",
    "num_hidden_layer=20\n",
    "\n",
    "\n",
    "# 添加L1正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(0.01 ))\n",
    "\n",
    "# 添加L2正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "decoding_dim=256\n",
    "input_layer = Input(shape=(630,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoder_layer)\n",
    "\n",
    "# 自编码器模型\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))  # 解码层\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "# 训练自编码器模型，使用训练集的特征进行训练和验证\n",
    "autoencoder.fit(features_train, features_train, epochs=100,batch_size=512, shuffle=True, validation_data=(features_test, features_test))\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "encoded_features_test = encoder.predict(features_test)\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "\n",
    "# 转换为Pandas DataFrame\n",
    "encoded_features_train_df = pd.DataFrame(encoded_features_train)\n",
    "\n",
    "\n",
    "# 使用降维后的特征训练预测模型，这里我们使用一个简单的线性回归模型作为例子\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 对测试集进行预测并评估模型性能\n",
    "predictions = regressor.predict(encoded_features_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "print('Mean Squared Error on test set: ', mse)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 使用降维后的特征训练线性回归模型\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 获取线性回归模型的参数\n",
    "coefficients = regressor.coef_  # 回归系数，对应每个特征的权重\n",
    "intercept = regressor.intercept_  # 截距项\n",
    "\n",
    "# 打印每个特征的回归系数\n",
    "for i, coef in enumerate(coefficients):\n",
    "    print(f'Coefficient for encoded feature {i+1}: {coef}')\n",
    "\n",
    "# 打印截距项\n",
    "print(f'Intercept: {intercept}')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 添加常数项，对应线性回归模型的截距\n",
    "encoded_features_train_with_intercept = sm.add_constant(encoded_features_train)\n",
    "\n",
    "# 使用statsmodels的OLS函数训练线性回归模型\n",
    "model = sm.OLS(target_train, encoded_features_train_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# 打印每个特征的回归系数、标准误差和p值\n",
    "print(results.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6435b35f-df9d-4192-9b6f-758c23afa121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1604 - val_loss: 0.1541\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1452 - val_loss: 0.1344\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.1211 - val_loss: 0.1065\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0916 - val_loss: 0.0767\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0640 - val_loss: 0.0523\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0353\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.0242\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0205 - val_loss: 0.0173\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0150 - val_loss: 0.0131\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0095 - val_loss: 0.0087\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0074\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 25/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 26/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 27/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 28/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 29/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 30/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 31/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 32/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 33/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 34/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 35/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 36/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 37/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 38/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 39/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 40/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 41/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 42/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 43/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 44/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 45/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 46/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 47/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 48/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 49/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 50/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 51/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 52/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 53/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 54/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 55/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 56/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 57/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 58/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 59/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 60/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 61/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 62/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 63/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 64/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 65/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 66/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 67/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 68/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 69/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 70/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 71/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 72/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 74/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 75/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 76/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 77/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 78/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 79/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 80/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 81/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 82/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 83/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 84/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 85/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 86/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 87/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 88/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 89/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 90/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 91/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 92/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 93/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 94/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 95/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 96/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 97/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 9.9482e-04 - val_loss: 9.9836e-04\n",
      "Epoch 98/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 9.7672e-04 - val_loss: 9.8011e-04\n",
      "Epoch 99/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 9.5904e-04 - val_loss: 9.6215e-04\n",
      "Epoch 100/100\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 9.4162e-04 - val_loss: 9.4449e-04\n",
      "1964/1964 [==============================] - 1s 428us/step\n",
      "491/491 [==============================] - 0s 459us/step\n",
      "1964/1964 [==============================] - 1s 430us/step\n",
      "Mean Squared Error on test set:  0.0003614477985068315\n",
      "Coefficient for encoded feature 1: 0.026745641604065895\n",
      "Coefficient for encoded feature 2: -0.03678319603204727\n",
      "Coefficient for encoded feature 3: -0.013419659808278084\n",
      "Coefficient for encoded feature 4: -0.00913203414529562\n",
      "Coefficient for encoded feature 5: 0.0010644332505762577\n",
      "Coefficient for encoded feature 6: -0.06686173379421234\n",
      "Coefficient for encoded feature 7: 0.016857005655765533\n",
      "Coefficient for encoded feature 8: 0.03746742010116577\n",
      "Coefficient for encoded feature 9: -0.09480879455804825\n",
      "Coefficient for encoded feature 10: 0.015144560486078262\n",
      "Coefficient for encoded feature 11: 0.008295671083033085\n",
      "Coefficient for encoded feature 12: 2.177020311355591\n",
      "Coefficient for encoded feature 13: 0.006996540352702141\n",
      "Coefficient for encoded feature 14: -0.032595038414001465\n",
      "Coefficient for encoded feature 15: -0.059342920780181885\n",
      "Coefficient for encoded feature 16: -0.038373466581106186\n",
      "Coefficient for encoded feature 17: 0.004828077740967274\n",
      "Coefficient for encoded feature 18: -0.042150091379880905\n",
      "Coefficient for encoded feature 19: -0.000758838839828968\n",
      "Coefficient for encoded feature 20: 0.0003317571245133877\n",
      "Coefficient for encoded feature 21: -0.04497484117746353\n",
      "Coefficient for encoded feature 22: 0.13923363387584686\n",
      "Coefficient for encoded feature 23: 0.042861223220825195\n",
      "Coefficient for encoded feature 24: 0.026079416275024414\n",
      "Coefficient for encoded feature 25: 0.05545758455991745\n",
      "Coefficient for encoded feature 26: -0.12929292023181915\n",
      "Coefficient for encoded feature 27: 0.07020862400531769\n",
      "Coefficient for encoded feature 28: -0.00011890009045600891\n",
      "Coefficient for encoded feature 29: 0.04113233834505081\n",
      "Coefficient for encoded feature 30: 0.00611237995326519\n",
      "Coefficient for encoded feature 31: 0.006862403824925423\n",
      "Coefficient for encoded feature 32: -0.04734569042921066\n",
      "Coefficient for encoded feature 33: 0.005929522216320038\n",
      "Coefficient for encoded feature 34: 0.008073657751083374\n",
      "Coefficient for encoded feature 35: 0.04256414622068405\n",
      "Coefficient for encoded feature 36: -0.04591205716133118\n",
      "Coefficient for encoded feature 37: 0.05679866671562195\n",
      "Coefficient for encoded feature 38: -0.06380771100521088\n",
      "Coefficient for encoded feature 39: 0.019962549209594727\n",
      "Coefficient for encoded feature 40: 0.034112006425857544\n",
      "Coefficient for encoded feature 41: 0.02821892499923706\n",
      "Coefficient for encoded feature 42: -0.051989421248435974\n",
      "Coefficient for encoded feature 43: 2.3402459621429443\n",
      "Coefficient for encoded feature 44: -0.009525001049041748\n",
      "Coefficient for encoded feature 45: -0.025534600019454956\n",
      "Coefficient for encoded feature 46: 0.1054181382060051\n",
      "Coefficient for encoded feature 47: 0.06082548201084137\n",
      "Coefficient for encoded feature 48: 0.02595822513103485\n",
      "Coefficient for encoded feature 49: -0.018020562827587128\n",
      "Coefficient for encoded feature 50: 0.038183122873306274\n",
      "Coefficient for encoded feature 51: 3.2708048820495605e-05\n",
      "Coefficient for encoded feature 52: 0.017440877854824066\n",
      "Coefficient for encoded feature 53: -0.002779543399810791\n",
      "Coefficient for encoded feature 54: 0.028557904064655304\n",
      "Coefficient for encoded feature 55: 0.02758225053548813\n",
      "Coefficient for encoded feature 56: -0.026428669691085815\n",
      "Coefficient for encoded feature 57: -0.013686150312423706\n",
      "Coefficient for encoded feature 58: 0.08137986063957214\n",
      "Coefficient for encoded feature 59: 0.09695260226726532\n",
      "Coefficient for encoded feature 60: -0.014150157570838928\n",
      "Coefficient for encoded feature 61: 0.0020621642470359802\n",
      "Coefficient for encoded feature 62: -0.06306807696819305\n",
      "Coefficient for encoded feature 63: -0.007988467812538147\n",
      "Coefficient for encoded feature 64: 0.02954857423901558\n",
      "Coefficient for encoded feature 65: -1.4260802268981934\n",
      "Coefficient for encoded feature 66: -0.09332379698753357\n",
      "Coefficient for encoded feature 67: -4.023313522338867e-06\n",
      "Coefficient for encoded feature 68: -0.10362088680267334\n",
      "Coefficient for encoded feature 69: -2.864748239517212e-05\n",
      "Coefficient for encoded feature 70: 0.037034839391708374\n",
      "Coefficient for encoded feature 71: 0.017537027597427368\n",
      "Coefficient for encoded feature 72: 0.04608771204948425\n",
      "Coefficient for encoded feature 73: -0.05800960958003998\n",
      "Coefficient for encoded feature 74: 0.023239687085151672\n",
      "Coefficient for encoded feature 75: -0.05600474029779434\n",
      "Coefficient for encoded feature 76: 0.1000610738992691\n",
      "Coefficient for encoded feature 77: 0.026448041200637817\n",
      "Coefficient for encoded feature 78: 1.1792478561401367\n",
      "Coefficient for encoded feature 79: 0.016909778118133545\n",
      "Coefficient for encoded feature 80: -0.07131695747375488\n",
      "Coefficient for encoded feature 81: 0.004200445953756571\n",
      "Coefficient for encoded feature 82: 0.03583718091249466\n",
      "Coefficient for encoded feature 83: 0.013835340738296509\n",
      "Coefficient for encoded feature 84: -0.054787106812000275\n",
      "Coefficient for encoded feature 85: 0.02649529278278351\n",
      "Coefficient for encoded feature 86: -0.034210205078125\n",
      "Coefficient for encoded feature 87: -0.0161362886428833\n",
      "Coefficient for encoded feature 88: 0.038795068860054016\n",
      "Coefficient for encoded feature 89: 0.048021167516708374\n",
      "Coefficient for encoded feature 90: 0.005411781370639801\n",
      "Coefficient for encoded feature 91: 0.0019480511546134949\n",
      "Coefficient for encoded feature 92: 0.15217986702919006\n",
      "Coefficient for encoded feature 93: -0.057893723249435425\n",
      "Coefficient for encoded feature 94: 0.0716928169131279\n",
      "Coefficient for encoded feature 95: -0.07731163501739502\n",
      "Coefficient for encoded feature 96: -0.0251009464263916\n",
      "Coefficient for encoded feature 97: 0.052513137459754944\n",
      "Coefficient for encoded feature 98: 0.010610319674015045\n",
      "Coefficient for encoded feature 99: 0.08004449307918549\n",
      "Coefficient for encoded feature 100: 0.028445571660995483\n",
      "Coefficient for encoded feature 101: -0.066610187292099\n",
      "Coefficient for encoded feature 102: 0.026415053755044937\n",
      "Coefficient for encoded feature 103: -0.008098185062408447\n",
      "Coefficient for encoded feature 104: -0.14795726537704468\n",
      "Coefficient for encoded feature 105: -0.08126448094844818\n",
      "Coefficient for encoded feature 106: 0.002491571009159088\n",
      "Coefficient for encoded feature 107: 0.06521931290626526\n",
      "Coefficient for encoded feature 108: 4.2110681533813477e-05\n",
      "Coefficient for encoded feature 109: 4.4986605644226074e-05\n",
      "Coefficient for encoded feature 110: 0.10975468158721924\n",
      "Coefficient for encoded feature 111: -1.4230608940124512e-05\n",
      "Coefficient for encoded feature 112: 0.014661960303783417\n",
      "Coefficient for encoded feature 113: 0.00047889165580272675\n",
      "Coefficient for encoded feature 114: 0.04281696677207947\n",
      "Coefficient for encoded feature 115: 0.0002619922161102295\n",
      "Coefficient for encoded feature 116: 0.035032644867897034\n",
      "Coefficient for encoded feature 117: -0.02099950611591339\n",
      "Coefficient for encoded feature 118: -0.03125918284058571\n",
      "Coefficient for encoded feature 119: -0.07337413728237152\n",
      "Coefficient for encoded feature 120: 0.07766954600811005\n",
      "Coefficient for encoded feature 121: -0.06977581977844238\n",
      "Coefficient for encoded feature 122: -0.007942244410514832\n",
      "Coefficient for encoded feature 123: 3.135204315185547e-05\n",
      "Coefficient for encoded feature 124: 0.03644584119319916\n",
      "Coefficient for encoded feature 125: 0.08509804308414459\n",
      "Coefficient for encoded feature 126: -0.033953532576560974\n",
      "Coefficient for encoded feature 127: -2.4437904357910156e-06\n",
      "Coefficient for encoded feature 128: -0.07467338442802429\n",
      "Coefficient for encoded feature 129: 0.0031253769993782043\n",
      "Coefficient for encoded feature 130: -0.0460682213306427\n",
      "Coefficient for encoded feature 131: -0.002117946743965149\n",
      "Coefficient for encoded feature 132: 3.039836883544922e-05\n",
      "Coefficient for encoded feature 133: -0.06384256482124329\n",
      "Coefficient for encoded feature 134: -0.025307893753051758\n",
      "Coefficient for encoded feature 135: -0.10821305215358734\n",
      "Coefficient for encoded feature 136: -0.11321882158517838\n",
      "Coefficient for encoded feature 137: 0.053966350853443146\n",
      "Coefficient for encoded feature 138: 0.0466984286904335\n",
      "Coefficient for encoded feature 139: 0.016976431012153625\n",
      "Coefficient for encoded feature 140: -0.0958109200000763\n",
      "Coefficient for encoded feature 141: 0.021950650960206985\n",
      "Coefficient for encoded feature 142: 7.808208465576172e-06\n",
      "Coefficient for encoded feature 143: 0.05029711127281189\n",
      "Coefficient for encoded feature 144: 0.06198896840214729\n",
      "Coefficient for encoded feature 145: 0.10922599583864212\n",
      "Coefficient for encoded feature 146: 0.005961745977401733\n",
      "Coefficient for encoded feature 147: 0.04409310221672058\n",
      "Coefficient for encoded feature 148: -0.0036676377058029175\n",
      "Coefficient for encoded feature 149: -0.003679513931274414\n",
      "Coefficient for encoded feature 150: -0.027173854410648346\n",
      "Coefficient for encoded feature 151: -0.02770403027534485\n",
      "Coefficient for encoded feature 152: -0.028336450457572937\n",
      "Coefficient for encoded feature 153: -0.03693777322769165\n",
      "Coefficient for encoded feature 154: 0.032984018325805664\n",
      "Coefficient for encoded feature 155: 1.0512769222259521e-05\n",
      "Coefficient for encoded feature 156: -0.07722613960504532\n",
      "Coefficient for encoded feature 157: -0.005881482735276222\n",
      "Coefficient for encoded feature 158: -6.854534149169922e-06\n",
      "Coefficient for encoded feature 159: 0.12093734741210938\n",
      "Coefficient for encoded feature 160: 0.003989040851593018\n",
      "Coefficient for encoded feature 161: -1.7881393432617188e-06\n",
      "Coefficient for encoded feature 162: 0.03994414210319519\n",
      "Coefficient for encoded feature 163: -0.008325930684804916\n",
      "Coefficient for encoded feature 164: -0.359430193901062\n",
      "Coefficient for encoded feature 165: 4.775822162628174e-06\n",
      "Coefficient for encoded feature 166: -0.006349116563796997\n",
      "Coefficient for encoded feature 167: 0.1049538180232048\n",
      "Coefficient for encoded feature 168: 0.034330904483795166\n",
      "Coefficient for encoded feature 169: 0.005165413022041321\n",
      "Coefficient for encoded feature 170: -0.10565111041069031\n",
      "Coefficient for encoded feature 171: -0.0054681673645973206\n",
      "Coefficient for encoded feature 172: 0.005649063736200333\n",
      "Coefficient for encoded feature 173: 0.06335465610027313\n",
      "Coefficient for encoded feature 174: -0.139511376619339\n",
      "Coefficient for encoded feature 175: -3.039836883544922e-06\n",
      "Coefficient for encoded feature 176: -3.591179847717285e-06\n",
      "Coefficient for encoded feature 177: 9.119510650634766e-06\n",
      "Coefficient for encoded feature 178: -0.062051139771938324\n",
      "Coefficient for encoded feature 179: -8.27014446258545e-06\n",
      "Coefficient for encoded feature 180: 0.0046194568276405334\n",
      "Coefficient for encoded feature 181: 0.0018049627542495728\n",
      "Coefficient for encoded feature 182: -0.0242486372590065\n",
      "Coefficient for encoded feature 183: 0.22209519147872925\n",
      "Coefficient for encoded feature 184: -0.0681261420249939\n",
      "Coefficient for encoded feature 185: 0.022992655634880066\n",
      "Coefficient for encoded feature 186: 0.03166304528713226\n",
      "Coefficient for encoded feature 187: 0.06458743661642075\n",
      "Coefficient for encoded feature 188: -0.04706194996833801\n",
      "Coefficient for encoded feature 189: 0.0003068596124649048\n",
      "Coefficient for encoded feature 190: 1.1362135410308838e-06\n",
      "Coefficient for encoded feature 191: -0.018043965101242065\n",
      "Coefficient for encoded feature 192: 0.060186877846717834\n",
      "Coefficient for encoded feature 193: 0.0011796653270721436\n",
      "Coefficient for encoded feature 194: 0.09043315798044205\n",
      "Coefficient for encoded feature 195: -0.016519814729690552\n",
      "Coefficient for encoded feature 196: 0.014136016368865967\n",
      "Coefficient for encoded feature 197: 1.7881393432617188e-07\n",
      "Coefficient for encoded feature 198: -0.023790091276168823\n",
      "Coefficient for encoded feature 199: 0.008246511220932007\n",
      "Coefficient for encoded feature 200: -0.03567337989807129\n",
      "Coefficient for encoded feature 201: -0.03195241093635559\n",
      "Coefficient for encoded feature 202: 0.03683693706989288\n",
      "Coefficient for encoded feature 203: -0.112179234623909\n",
      "Coefficient for encoded feature 204: 1.1175870895385742e-08\n",
      "Coefficient for encoded feature 205: 0.004533484578132629\n",
      "Coefficient for encoded feature 206: -0.01078978180885315\n",
      "Coefficient for encoded feature 207: -0.10850971937179565\n",
      "Coefficient for encoded feature 208: -3.725290298461914e-09\n",
      "Coefficient for encoded feature 209: -0.00629250705242157\n",
      "Coefficient for encoded feature 210: 0.006918102502822876\n",
      "Coefficient for encoded feature 211: -0.05379471182823181\n",
      "Coefficient for encoded feature 212: 0.04312513768672943\n",
      "Coefficient for encoded feature 213: 0.10689346492290497\n",
      "Coefficient for encoded feature 214: 0.0\n",
      "Coefficient for encoded feature 215: -0.007452208548784256\n",
      "Coefficient for encoded feature 216: -0.009009182453155518\n",
      "Coefficient for encoded feature 217: 0.07453516125679016\n",
      "Coefficient for encoded feature 218: -0.056777000427246094\n",
      "Coefficient for encoded feature 219: 0.03839990496635437\n",
      "Coefficient for encoded feature 220: -0.07178433239459991\n",
      "Coefficient for encoded feature 221: 0.005496025085449219\n",
      "Coefficient for encoded feature 222: -0.10770154744386673\n",
      "Coefficient for encoded feature 223: -0.027256090193986893\n",
      "Coefficient for encoded feature 224: -4.066465854644775\n",
      "Coefficient for encoded feature 225: -0.031613513827323914\n",
      "Coefficient for encoded feature 226: -0.006669357419013977\n",
      "Coefficient for encoded feature 227: 0.0\n",
      "Coefficient for encoded feature 228: 0.03206697851419449\n",
      "Coefficient for encoded feature 229: -0.02609393000602722\n",
      "Coefficient for encoded feature 230: -0.05562102794647217\n",
      "Coefficient for encoded feature 231: 0.04194815829396248\n",
      "Coefficient for encoded feature 232: 0.08031529188156128\n",
      "Coefficient for encoded feature 233: 0.1154680922627449\n",
      "Coefficient for encoded feature 234: -0.03349290043115616\n",
      "Coefficient for encoded feature 235: 0.004105508327484131\n",
      "Coefficient for encoded feature 236: -0.07421484589576721\n",
      "Coefficient for encoded feature 237: 0.008931785821914673\n",
      "Coefficient for encoded feature 238: -0.028012670576572418\n",
      "Coefficient for encoded feature 239: -0.004953354597091675\n",
      "Coefficient for encoded feature 240: -0.06806449592113495\n",
      "Intercept: 0.013279295526444912\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Return   R-squared:                       0.242\n",
      "Model:                            OLS   Adj. R-squared:                  0.240\n",
      "Method:                 Least Squares   F-statistic:                     94.82\n",
      "Date:                Sun, 15 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        21:48:20   Log-Likelihood:             1.5873e+05\n",
      "No. Observations:               62845   AIC:                        -3.170e+05\n",
      "Df Residuals:                   62633   BIC:                        -3.151e+05\n",
      "Df Model:                         211                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0133      0.019      0.688      0.491      -0.025       0.051\n",
      "x1             0.0267      0.016      1.655      0.098      -0.005       0.058\n",
      "x2            -0.0368      0.012     -3.152      0.002      -0.060      -0.014\n",
      "x3            -0.0134      0.016     -0.856      0.392      -0.044       0.017\n",
      "x4            -0.0091      0.016     -0.577      0.564      -0.040       0.022\n",
      "x5          2.389e-13   3.08e-13      0.776      0.438   -3.64e-13    8.42e-13\n",
      "x6            -0.0669      0.019     -3.606      0.000      -0.103      -0.031\n",
      "x7             0.0169      0.013      1.324      0.186      -0.008       0.042\n",
      "x8             0.0375      0.017      2.238      0.025       0.005       0.070\n",
      "x9            -0.0948      0.018     -5.312      0.000      -0.130      -0.060\n",
      "x10            0.0151      0.012      1.220      0.222      -0.009       0.039\n",
      "x11            0.0083      0.005      1.583      0.113      -0.002       0.019\n",
      "x12            2.1769      0.452      4.812      0.000       1.290       3.064\n",
      "x13            0.0070      0.016      0.435      0.664      -0.025       0.039\n",
      "x14           -0.0326      0.016     -2.019      0.044      -0.064      -0.001\n",
      "x15           -0.0593      0.019     -3.078      0.002      -0.097      -0.022\n",
      "x16           -0.0384      0.015     -2.477      0.013      -0.069      -0.008\n",
      "x17            0.0048      0.016      0.306      0.760      -0.026       0.036\n",
      "x18           -0.0421      0.019     -2.222      0.026      -0.079      -0.005\n",
      "x19         -1.02e-13   2.02e-13     -0.504      0.614   -4.99e-13    2.95e-13\n",
      "x20         4.036e-14   4.04e-13      0.100      0.920   -7.51e-13    8.31e-13\n",
      "x21           -0.0450      0.018     -2.539      0.011      -0.080      -0.010\n",
      "x22            0.1392      0.018      7.549      0.000       0.103       0.175\n",
      "x23            0.0429      0.016      2.603      0.009       0.011       0.075\n",
      "x24            0.0261      0.017      1.569      0.117      -0.006       0.059\n",
      "x25            0.0555      0.017      3.274      0.001       0.022       0.089\n",
      "x26           -0.1293      0.015     -8.738      0.000      -0.158      -0.100\n",
      "x27            0.0702      0.017      4.188      0.000       0.037       0.103\n",
      "x28         1.764e-13   3.94e-13      0.448      0.654   -5.96e-13    9.49e-13\n",
      "x29            0.0411      0.016      2.525      0.012       0.009       0.073\n",
      "x30            0.0061      0.017      0.356      0.722      -0.028       0.040\n",
      "x31            0.0069      0.017      0.417      0.676      -0.025       0.039\n",
      "x32           -0.0474      0.015     -3.186      0.001      -0.076      -0.018\n",
      "x33            0.0059      0.002      2.693      0.007       0.002       0.010\n",
      "x34            0.0081      0.019      0.425      0.671      -0.029       0.045\n",
      "x35            0.0426      0.008      5.509      0.000       0.027       0.058\n",
      "x36           -0.0459      0.014     -3.377      0.001      -0.073      -0.019\n",
      "x37            0.0568      0.016      3.489      0.000       0.025       0.089\n",
      "x38           -0.0638      0.017     -3.849      0.000      -0.096      -0.031\n",
      "x39            0.0200      0.007      2.700      0.007       0.005       0.034\n",
      "x40            0.0341      0.015      2.298      0.022       0.005       0.063\n",
      "x41            0.0282      0.015      1.927      0.054      -0.000       0.057\n",
      "x42           -0.0520      0.019     -2.706      0.007      -0.090      -0.014\n",
      "x43            2.3401      0.486      4.812      0.000       1.387       3.293\n",
      "x44           -0.0095      0.004     -2.130      0.033      -0.018      -0.001\n",
      "x45           -0.0255      0.015     -1.672      0.095      -0.055       0.004\n",
      "x46            0.1054      0.016      6.730      0.000       0.075       0.136\n",
      "x47            0.0608      0.019      3.206      0.001       0.024       0.098\n",
      "x48            0.0260      0.014      1.810      0.070      -0.002       0.054\n",
      "x49           -0.0180      0.011     -1.619      0.105      -0.040       0.004\n",
      "x50            0.0382      0.017      2.209      0.027       0.004       0.072\n",
      "x51         5.156e-14   1.38e-13      0.374      0.709   -2.19e-13    3.22e-13\n",
      "x52            0.0174      0.017      1.020      0.308      -0.016       0.051\n",
      "x53           -0.0028      0.004     -0.694      0.488      -0.011       0.005\n",
      "x54            0.0286      0.018      1.581      0.114      -0.007       0.064\n",
      "x55            0.0276      0.018      1.549      0.121      -0.007       0.062\n",
      "x56           -0.0264      0.013     -2.100      0.036      -0.051      -0.002\n",
      "x57           -0.0137      0.019     -0.707      0.479      -0.052       0.024\n",
      "x58            0.0814      0.017      4.775      0.000       0.048       0.115\n",
      "x59            0.0970      0.016      5.976      0.000       0.065       0.129\n",
      "x60           -0.0142      0.014     -1.017      0.309      -0.041       0.013\n",
      "x61            0.0021      0.004      0.495      0.621      -0.006       0.010\n",
      "x62           -0.0631      0.015     -4.118      0.000      -0.093      -0.033\n",
      "x63           -0.0080      0.017     -0.465      0.642      -0.042       0.026\n",
      "x64            0.0295      0.015      1.955      0.051   -7.75e-05       0.059\n",
      "x65           -1.4261      0.792     -1.800      0.072      -2.979       0.127\n",
      "x66           -0.0933      0.018     -5.215      0.000      -0.128      -0.058\n",
      "x67        -5.228e-14    3.9e-14     -1.342      0.180   -1.29e-13    2.41e-14\n",
      "x68           -0.1037      0.015     -6.872      0.000      -0.133      -0.074\n",
      "x69         2.901e-14    7.6e-14      0.382      0.703    -1.2e-13    1.78e-13\n",
      "x70            0.0370      0.011      3.492      0.000       0.016       0.058\n",
      "x71            0.0175      0.017      1.030      0.303      -0.016       0.051\n",
      "x72            0.0461      0.013      3.666      0.000       0.021       0.071\n",
      "x73           -0.0580      0.015     -3.942      0.000      -0.087      -0.029\n",
      "x74            0.0232      0.015      1.599      0.110      -0.005       0.052\n",
      "x75           -0.0560      0.015     -3.664      0.000      -0.086      -0.026\n",
      "x76            0.1001      0.016      6.299      0.000       0.069       0.131\n",
      "x77            0.0265      0.020      1.342      0.180      -0.012       0.065\n",
      "x78            1.1799      3.847      0.307      0.759      -6.360       8.720\n",
      "x79            0.0169      0.019      0.900      0.368      -0.020       0.054\n",
      "x80           -0.0713      0.015     -4.616      0.000      -0.102      -0.041\n",
      "x81            0.0042      0.036      0.117      0.907      -0.067       0.075\n",
      "x82            0.0358      0.016      2.267      0.023       0.005       0.067\n",
      "x83            0.0138      0.016      0.865      0.387      -0.017       0.045\n",
      "x84           -0.0548      0.018     -3.095      0.002      -0.090      -0.020\n",
      "x85            0.0265      0.017      1.516      0.129      -0.008       0.061\n",
      "x86           -0.0342      0.016     -2.121      0.034      -0.066      -0.003\n",
      "x87           -0.0161      0.004     -3.817      0.000      -0.024      -0.008\n",
      "x88            0.0388      0.015      2.590      0.010       0.009       0.068\n",
      "x89            0.0480      0.016      3.003      0.003       0.017       0.079\n",
      "x90            0.0054      0.019      0.290      0.772      -0.031       0.042\n",
      "x91            0.0019      0.007      0.267      0.789      -0.012       0.016\n",
      "x92            0.1522      0.017      9.004      0.000       0.119       0.185\n",
      "x93           -0.0579      0.015     -3.813      0.000      -0.088      -0.028\n",
      "x94            0.0717      0.016      4.433      0.000       0.040       0.103\n",
      "x95           -0.0773      0.015     -5.056      0.000      -0.107      -0.047\n",
      "x96           -0.0251      0.008     -3.146      0.002      -0.041      -0.009\n",
      "x97            0.0525      0.017      3.136      0.002       0.020       0.085\n",
      "x98            0.0106      0.015      0.694      0.487      -0.019       0.041\n",
      "x99            0.0800      0.015      5.171      0.000       0.050       0.110\n",
      "x100           0.0284      0.017      1.637      0.102      -0.006       0.062\n",
      "x101          -0.0666      0.014     -4.792      0.000      -0.094      -0.039\n",
      "x102           0.0264      0.018      1.477      0.140      -0.009       0.061\n",
      "x103          -0.0081      0.015     -0.528      0.597      -0.038       0.022\n",
      "x104          -0.1480      0.021     -7.136      0.000      -0.189      -0.107\n",
      "x105          -0.0813      0.028     -2.921      0.003      -0.136      -0.027\n",
      "x106           0.0025      0.016      0.158      0.875      -0.029       0.034\n",
      "x107           0.0652      0.017      3.807      0.000       0.032       0.099\n",
      "x108       -3.789e-14   1.68e-13     -0.226      0.821   -3.66e-13     2.9e-13\n",
      "x109       -6.732e-15    1.1e-13     -0.061      0.951   -2.22e-13    2.08e-13\n",
      "x110           0.1098      0.016      7.040      0.000       0.079       0.140\n",
      "x111        2.042e-15   5.21e-15      0.392      0.695   -8.17e-15    1.23e-14\n",
      "x112           0.0146      0.019      0.753      0.451      -0.023       0.053\n",
      "x113           0.0005      0.016      0.028      0.978      -0.032       0.033\n",
      "x114           0.0428      0.011      3.870      0.000       0.021       0.064\n",
      "x115           0.0003      0.010      0.027      0.979      -0.019       0.019\n",
      "x116           0.0350      0.017      2.089      0.037       0.002       0.068\n",
      "x117          -0.0210      0.015     -1.423      0.155      -0.050       0.008\n",
      "x118          -0.0313      0.006     -5.109      0.000      -0.043      -0.019\n",
      "x119          -0.0734      0.018     -4.039      0.000      -0.109      -0.038\n",
      "x120           0.0777      0.016      4.855      0.000       0.046       0.109\n",
      "x121          -0.0698      0.017     -4.127      0.000      -0.103      -0.037\n",
      "x122          -0.0079      0.016     -0.485      0.628      -0.040       0.024\n",
      "x123        1.847e-14   1.59e-13      0.116      0.908   -2.93e-13     3.3e-13\n",
      "x124           0.0364      0.016      2.245      0.025       0.005       0.068\n",
      "x125           0.0851      0.018      4.695      0.000       0.050       0.121\n",
      "x126          -0.0339      0.016     -2.145      0.032      -0.065      -0.003\n",
      "x127         -4.6e-14   1.93e-13     -0.239      0.811   -4.24e-13    3.32e-13\n",
      "x128          -0.0747      0.016     -4.663      0.000      -0.106      -0.043\n",
      "x129           0.0032      0.016      0.200      0.842      -0.028       0.034\n",
      "x130          -0.0460      0.018     -2.596      0.009      -0.081      -0.011\n",
      "x131          -0.0021      0.005     -0.415      0.678      -0.012       0.008\n",
      "x132       -3.014e-15   8.57e-15     -0.352      0.725   -1.98e-14    1.38e-14\n",
      "x133          -0.0639      0.017     -3.765      0.000      -0.097      -0.031\n",
      "x134          -0.0253      0.020     -1.284      0.199      -0.064       0.013\n",
      "x135          -0.1082      0.017     -6.359      0.000      -0.142      -0.075\n",
      "x136          -0.1132      0.018     -6.224      0.000      -0.149      -0.078\n",
      "x137           0.0540      0.018      2.979      0.003       0.018       0.089\n",
      "x138           0.0467      0.016      2.910      0.004       0.015       0.078\n",
      "x139           0.0170      0.015      1.109      0.267      -0.013       0.047\n",
      "x140          -0.0958      0.019     -4.926      0.000      -0.134      -0.058\n",
      "x141           0.0220      0.018      1.209      0.227      -0.014       0.058\n",
      "x142       -1.492e-14   6.31e-14     -0.236      0.813   -1.39e-13    1.09e-13\n",
      "x143           0.0503      0.021      2.443      0.015       0.010       0.091\n",
      "x144           0.0620      0.019      3.330      0.001       0.025       0.098\n",
      "x145           0.1092      0.018      6.183      0.000       0.075       0.144\n",
      "x146           0.0060      0.015      0.409      0.683      -0.023       0.035\n",
      "x147           0.0441      0.020      2.211      0.027       0.005       0.083\n",
      "x148          -0.0037      0.002     -1.682      0.093      -0.008       0.001\n",
      "x149          -0.0037      0.015     -0.252      0.801      -0.032       0.025\n",
      "x150          -0.0272      0.016     -1.702      0.089      -0.058       0.004\n",
      "x151          -0.0277      0.014     -1.976      0.048      -0.055      -0.000\n",
      "x152          -0.0283      0.013     -2.122      0.034      -0.055      -0.002\n",
      "x153          -0.0369      0.016     -2.369      0.018      -0.068      -0.006\n",
      "x154           0.0330      0.017      1.998      0.046       0.001       0.065\n",
      "x155        2.376e-14   4.88e-14      0.487      0.626   -7.19e-14    1.19e-13\n",
      "x156          -0.0772      0.020     -3.931      0.000      -0.116      -0.039\n",
      "x157          -0.0059      0.017     -0.340      0.734      -0.040       0.028\n",
      "x158        6.867e-15   3.14e-14      0.219      0.827   -5.46e-14    6.83e-14\n",
      "x159           0.1210      0.016      7.799      0.000       0.091       0.151\n",
      "x160           0.0040      0.017      0.230      0.818      -0.030       0.038\n",
      "x161        2.414e-14   5.29e-14      0.457      0.648   -7.95e-14    1.28e-13\n",
      "x162           0.0400      0.016      2.442      0.015       0.008       0.072\n",
      "x163          -0.0083      0.021     -0.394      0.693      -0.050       0.033\n",
      "x164          -0.3596      0.202     -1.779      0.075      -0.756       0.037\n",
      "x165       -3.655e-14   4.54e-14     -0.805      0.421   -1.26e-13    5.25e-14\n",
      "x166          -0.0063      0.019     -0.340      0.734      -0.043       0.030\n",
      "x167           0.1049      0.015      7.082      0.000       0.076       0.134\n",
      "x168           0.0343      0.016      2.108      0.035       0.002       0.066\n",
      "x169           0.0052      0.016      0.326      0.745      -0.026       0.036\n",
      "x170          -0.1056      0.017     -6.125      0.000      -0.139      -0.072\n",
      "x171          -0.0055      0.013     -0.420      0.674      -0.031       0.020\n",
      "x172           0.0056      0.002      2.285      0.022       0.001       0.010\n",
      "x173           0.0633      0.016      4.076      0.000       0.033       0.094\n",
      "x174          -0.1395      0.017     -8.338      0.000      -0.172      -0.107\n",
      "x175       -4.226e-15   1.04e-14     -0.406      0.685   -2.46e-14    1.62e-14\n",
      "x176        5.258e-15   4.83e-14      0.109      0.913   -8.94e-14    9.99e-14\n",
      "x177        2.695e-16   2.76e-14      0.010      0.992   -5.39e-14    5.44e-14\n",
      "x178          -0.0621      0.019     -3.303      0.001      -0.099      -0.025\n",
      "x179       -3.294e-14   5.42e-14     -0.608      0.543   -1.39e-13    7.32e-14\n",
      "x180           0.0046      0.014      0.339      0.735      -0.022       0.031\n",
      "x181           0.0018      0.004      0.495      0.621      -0.005       0.009\n",
      "x182          -0.0243      0.016     -1.539      0.124      -0.055       0.007\n",
      "x183           0.2210      0.218      1.012      0.311      -0.207       0.649\n",
      "x184          -0.0681      0.018     -3.828      0.000      -0.103      -0.033\n",
      "x185           0.0230      0.017      1.345      0.179      -0.011       0.056\n",
      "x186           0.0316      0.018      1.802      0.072      -0.003       0.066\n",
      "x187           0.0646      0.016      4.058      0.000       0.033       0.096\n",
      "x188          -0.0471      0.016     -2.960      0.003      -0.078      -0.016\n",
      "x189           0.0003      0.002      0.155      0.877      -0.004       0.004\n",
      "x190       -4.026e-15   1.49e-14     -0.269      0.788   -3.33e-14    2.53e-14\n",
      "x191          -0.0181      0.015     -1.166      0.243      -0.048       0.012\n",
      "x192           0.0602      0.016      3.858      0.000       0.030       0.091\n",
      "x193           0.0012      0.006      0.203      0.839      -0.010       0.013\n",
      "x194           0.0904      0.018      5.131      0.000       0.056       0.125\n",
      "x195          -0.0165      0.004     -3.713      0.000      -0.025      -0.008\n",
      "x196           0.0141      0.003      4.134      0.000       0.007       0.021\n",
      "x197       -2.025e-14    5.8e-14     -0.349      0.727   -1.34e-13    9.34e-14\n",
      "x198          -0.0238      0.020     -1.216      0.224      -0.062       0.015\n",
      "x199           0.0082      0.008      1.048      0.295      -0.007       0.024\n",
      "x200          -0.0357      0.017     -2.106      0.035      -0.069      -0.002\n",
      "x201          -0.0320      0.015     -2.198      0.028      -0.060      -0.003\n",
      "x202           0.0368      0.012      3.040      0.002       0.013       0.061\n",
      "x203          -0.1122      0.015     -7.379      0.000      -0.142      -0.082\n",
      "x204        4.559e-17   8.17e-17      0.558      0.577   -1.15e-16    2.06e-16\n",
      "x205           0.0045      0.014      0.319      0.750      -0.023       0.032\n",
      "x206          -0.0108      0.015     -0.721      0.471      -0.040       0.019\n",
      "x207          -0.1085      0.017     -6.574      0.000      -0.141      -0.076\n",
      "x208        1.298e-17   4.93e-18      2.635      0.008    3.32e-18    2.26e-17\n",
      "x209          -0.0063      0.002     -2.752      0.006      -0.011      -0.002\n",
      "x210           0.0069      0.016      0.421      0.674      -0.025       0.039\n",
      "x211          -0.0538      0.013     -4.103      0.000      -0.079      -0.028\n",
      "x212           0.0431      0.012      3.685      0.000       0.020       0.066\n",
      "x213           0.1069      0.016      6.599      0.000       0.075       0.139\n",
      "x214                0          0        nan        nan           0           0\n",
      "x215          -0.0074      0.018     -0.417      0.677      -0.042       0.027\n",
      "x216          -0.0090      0.008     -1.166      0.244      -0.024       0.006\n",
      "x217           0.0745      0.015      4.841      0.000       0.044       0.105\n",
      "x218          -0.0568      0.014     -4.069      0.000      -0.084      -0.029\n",
      "x219           0.0384      0.013      2.875      0.004       0.012       0.065\n",
      "x220          -0.0718      0.016     -4.442      0.000      -0.103      -0.040\n",
      "x221           0.0055      0.014      0.384      0.701      -0.023       0.034\n",
      "x222          -0.1077      0.015     -7.317      0.000      -0.137      -0.079\n",
      "x223          -0.0273      0.015     -1.851      0.064      -0.056       0.002\n",
      "x224          -4.0664      0.840     -4.843      0.000      -5.712      -2.421\n",
      "x225          -0.0316      0.016     -1.954      0.051      -0.063       0.000\n",
      "x226          -0.0067      0.015     -0.458      0.647      -0.035       0.022\n",
      "x227                0          0        nan        nan           0           0\n",
      "x228           0.0320      0.014      2.241      0.025       0.004       0.060\n",
      "x229          -0.0261      0.014     -1.882      0.060      -0.053       0.001\n",
      "x230          -0.0556      0.014     -3.863      0.000      -0.084      -0.027\n",
      "x231           0.0419      0.016      2.614      0.009       0.010       0.073\n",
      "x232           0.0803      0.020      3.948      0.000       0.040       0.120\n",
      "x233           0.1155      0.017      6.640      0.000       0.081       0.150\n",
      "x234          -0.0335      0.018     -1.867      0.062      -0.069       0.002\n",
      "x235           0.0041      0.010      0.422      0.673      -0.015       0.023\n",
      "x236          -0.0742      0.017     -4.489      0.000      -0.107      -0.042\n",
      "x237           0.0089      0.017      0.515      0.607      -0.025       0.043\n",
      "x238          -0.0280      0.009     -3.117      0.002      -0.046      -0.010\n",
      "x239          -0.0049      0.017     -0.298      0.766      -0.038       0.028\n",
      "x240          -0.0681      0.014     -4.821      0.000      -0.096      -0.040\n",
      "==============================================================================\n",
      "Omnibus:                    22087.414   Durbin-Watson:                   1.989\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           122457.724\n",
      "Skew:                           1.597   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.047   Cond. No.                     1.00e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.94e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#设置隐藏因子的数量\n",
    "encoding_dim = 240\n",
    "hidden_dim=315\n",
    "num_hidden_layer=60\n",
    "\n",
    "\n",
    "# 添加L1正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(0.01 ))\n",
    "\n",
    "# 添加L2正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "decoding_dim=256\n",
    "input_layer = Input(shape=(630,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoder_layer)\n",
    "\n",
    "# 自编码器模型\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))  # 解码层\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# 训练自编码器模型，使用训练集的特征进行训练和验证\n",
    "autoencoder.fit(features_train, features_train, epochs=100,batch_size=512, shuffle=True, validation_data=(features_test, features_test))\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "encoded_features_test = encoder.predict(features_test)\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "\n",
    "# 转换为Pandas DataFrame\n",
    "encoded_features_train_df = pd.DataFrame(encoded_features_train)\n",
    "\n",
    "\n",
    "# 使用降维后的特征训练预测模型，这里我们使用一个简单的线性回归模型作为例子\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 对测试集进行预测并评估模型性能\n",
    "predictions = regressor.predict(encoded_features_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "print('Mean Squared Error on test set: ', mse)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 使用降维后的特征训练线性回归模型\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 获取线性回归模型的参数\n",
    "coefficients = regressor.coef_  # 回归系数，对应每个特征的权重\n",
    "intercept = regressor.intercept_  # 截距项\n",
    "\n",
    "# 打印每个特征的回归系数\n",
    "for i, coef in enumerate(coefficients):\n",
    "    print(f'Coefficient for encoded feature {i+1}: {coef}')\n",
    "\n",
    "# 打印截距项\n",
    "print(f'Intercept: {intercept}')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 添加常数项，对应线性回归模型的截距\n",
    "encoded_features_train_with_intercept = sm.add_constant(encoded_features_train)\n",
    "\n",
    "# 使用statsmodels的OLS函数训练线性回归模型\n",
    "model = sm.OLS(target_train, encoded_features_train_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# 打印每个特征的回归系数、标准误差和p值\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4f2077-cf06-43cc-b707-907a9f7d5f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1646 - val_loss: 0.1637\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1626 - val_loss: 0.1610\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1590 - val_loss: 0.1565\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1533 - val_loss: 0.1496\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1453 - val_loss: 0.1407\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1358 - val_loss: 0.1307\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1254 - val_loss: 0.1200\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.1090\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.0977\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0921 - val_loss: 0.0866\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0814 - val_loss: 0.0762\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.0667\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0623 - val_loss: 0.0581\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0543 - val_loss: 0.0506\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0440\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0411 - val_loss: 0.0384\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0335\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0293\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0258\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0227\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0202\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0179\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0160\n",
      "Epoch 24/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0143\n",
      "Epoch 25/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 26/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 27/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 28/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 29/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 30/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.0086\n",
      "Epoch 31/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0081\n",
      "Epoch 32/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 33/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 34/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 35/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 36/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 37/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 38/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 39/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 40/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 41/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 42/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 43/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 44/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 45/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 46/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 47/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 48/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 49/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 50/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 51/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 52/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 53/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 54/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 55/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 56/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 57/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 58/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 59/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 60/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 61/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 62/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 63/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 64/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 65/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 66/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 67/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 68/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 69/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 70/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 71/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 72/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 73/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 74/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 75/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 76/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 77/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 78/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 79/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 80/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 81/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 82/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 83/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 84/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 85/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 86/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 87/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 88/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 89/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 90/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 91/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 92/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 93/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 94/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 95/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 96/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 97/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 98/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 99/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 100/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "1964/1964 [==============================] - 1s 299us/step\n",
      "491/491 [==============================] - 0s 332us/step\n",
      "1964/1964 [==============================] - 1s 313us/step\n",
      "Mean Squared Error on test set:  0.00040343285963608455\n",
      "Coefficient for encoded feature 1: 0.055321548134088516\n",
      "Coefficient for encoded feature 2: 0.05010740086436272\n",
      "Coefficient for encoded feature 3: -0.01411454938352108\n",
      "Coefficient for encoded feature 4: -0.007943320088088512\n",
      "Coefficient for encoded feature 5: 0.020040709525346756\n",
      "Coefficient for encoded feature 6: 0.03337574377655983\n",
      "Coefficient for encoded feature 7: -0.03511174023151398\n",
      "Coefficient for encoded feature 8: -0.03432396054267883\n",
      "Coefficient for encoded feature 9: -0.005405642092227936\n",
      "Coefficient for encoded feature 10: 0.03547243773937225\n",
      "Coefficient for encoded feature 11: -0.00807125959545374\n",
      "Coefficient for encoded feature 12: -0.0016132006421685219\n",
      "Coefficient for encoded feature 13: -3.818422555923462e-08\n",
      "Coefficient for encoded feature 14: -0.01026841439306736\n",
      "Coefficient for encoded feature 15: 7.450580596923828e-09\n",
      "Coefficient for encoded feature 16: -0.028318963944911957\n",
      "Coefficient for encoded feature 17: -0.04730498045682907\n",
      "Coefficient for encoded feature 18: -0.06054625287652016\n",
      "Coefficient for encoded feature 19: 0.0625097006559372\n",
      "Coefficient for encoded feature 20: 0.09906644374132156\n",
      "Coefficient for encoded feature 21: 0.012424286454916\n",
      "Coefficient for encoded feature 22: 0.040910203009843826\n",
      "Coefficient for encoded feature 23: -0.026067085564136505\n",
      "Coefficient for encoded feature 24: -0.07667098194360733\n",
      "Coefficient for encoded feature 25: -5.587935447692871e-09\n",
      "Coefficient for encoded feature 26: 0.04314650595188141\n",
      "Coefficient for encoded feature 27: 0.02661651372909546\n",
      "Coefficient for encoded feature 28: 0.04487861692905426\n",
      "Coefficient for encoded feature 29: -0.07870355248451233\n",
      "Coefficient for encoded feature 30: -0.08340374380350113\n",
      "Intercept: 0.039164695888757706\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Return   R-squared:                       0.143\n",
      "Model:                            OLS   Adj. R-squared:                  0.143\n",
      "Method:                 Least Squares   F-statistic:                     388.0\n",
      "Date:                Sun, 15 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        21:49:17   Log-Likelihood:             1.5487e+05\n",
      "No. Observations:               62845   AIC:                        -3.097e+05\n",
      "Df Residuals:                   62817   BIC:                        -3.094e+05\n",
      "Df Model:                          27                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0392      0.003     12.416      0.000       0.033       0.045\n",
      "x1             0.0553      0.003     21.599      0.000       0.050       0.060\n",
      "x2             0.0501      0.003     17.778      0.000       0.045       0.056\n",
      "x3            -0.0141      0.003     -5.429      0.000      -0.019      -0.009\n",
      "x4            -0.0079      0.002     -4.207      0.000      -0.012      -0.004\n",
      "x5             0.0200      0.002      8.194      0.000       0.015       0.025\n",
      "x6             0.0334      0.002     14.061      0.000       0.029       0.038\n",
      "x7            -0.0351      0.003    -12.862      0.000      -0.040      -0.030\n",
      "x8            -0.0343      0.002    -15.754      0.000      -0.039      -0.030\n",
      "x9            -0.0054      0.001     -4.593      0.000      -0.008      -0.003\n",
      "x10            0.0355      0.003     13.445      0.000       0.030       0.041\n",
      "x11           -0.0081      0.002     -3.253      0.001      -0.013      -0.003\n",
      "x12           -0.0016      0.001     -1.151      0.250      -0.004       0.001\n",
      "x13        -7.154e-18   2.22e-18     -3.228      0.001   -1.15e-17   -2.81e-18\n",
      "x14           -0.0103      0.003     -3.392      0.001      -0.016      -0.004\n",
      "x15          -2.2e-17   2.76e-18     -7.964      0.000   -2.74e-17   -1.66e-17\n",
      "x16           -0.0283      0.003    -10.259      0.000      -0.034      -0.023\n",
      "x17           -0.0473      0.003    -15.174      0.000      -0.053      -0.041\n",
      "x18           -0.0605      0.002    -31.337      0.000      -0.064      -0.057\n",
      "x19            0.0625      0.003     24.858      0.000       0.058       0.067\n",
      "x20            0.0991      0.003     34.253      0.000       0.093       0.105\n",
      "x21            0.0124      0.002      5.882      0.000       0.008       0.017\n",
      "x22            0.0409      0.002     16.530      0.000       0.036       0.046\n",
      "x23           -0.0261      0.003     -8.209      0.000      -0.032      -0.020\n",
      "x24           -0.0767      0.002    -34.335      0.000      -0.081      -0.072\n",
      "x25         1.561e-17   1.56e-18      9.982      0.000    1.25e-17    1.87e-17\n",
      "x26            0.0431      0.002     17.426      0.000       0.038       0.048\n",
      "x27            0.0266      0.002     12.550      0.000       0.022       0.031\n",
      "x28            0.0449      0.003     15.347      0.000       0.039       0.051\n",
      "x29           -0.0787      0.003    -28.901      0.000      -0.084      -0.073\n",
      "x30           -0.0834      0.003    -29.621      0.000      -0.089      -0.078\n",
      "==============================================================================\n",
      "Omnibus:                    25180.756   Durbin-Watson:                   1.989\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           128672.235\n",
      "Skew:                           1.894   Prob(JB):                         0.00\n",
      "Kurtosis:                       8.899   Cond. No.                     1.00e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.91e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#设置隐藏因子的数量\n",
    "encoding_dim = 30\n",
    "hidden_dim=315\n",
    "num_hidden_layer=20\n",
    "\n",
    "\n",
    "# 添加L1正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(0.01 ))\n",
    "\n",
    "# 添加L2正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "decoding_dim=256\n",
    "input_layer = Input(shape=(630,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoder_layer)\n",
    "\n",
    "# 自编码器模型\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))  # 解码层\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "# 训练自编码器模型，使用训练集的特征进行训练和验证\n",
    "autoencoder.fit(features_train, features_train, epochs=100,batch_size=512, shuffle=True, validation_data=(features_test, features_test))\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "encoded_features_test = encoder.predict(features_test)\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "\n",
    "# 转换为Pandas DataFrame\n",
    "encoded_features_train_df = pd.DataFrame(encoded_features_train)\n",
    "\n",
    "\n",
    "# 使用降维后的特征训练预测模型，这里我们使用一个简单的线性回归模型作为例子\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 对测试集进行预测并评估模型性能\n",
    "predictions = regressor.predict(encoded_features_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "print('Mean Squared Error on test set: ', mse)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 使用降维后的特征训练线性回归模型\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 获取线性回归模型的参数\n",
    "coefficients = regressor.coef_  # 回归系数，对应每个特征的权重\n",
    "intercept = regressor.intercept_  # 截距项\n",
    "\n",
    "# 打印每个特征的回归系数\n",
    "for i, coef in enumerate(coefficients):\n",
    "    print(f'Coefficient for encoded feature {i+1}: {coef}')\n",
    "\n",
    "# 打印截距项\n",
    "print(f'Intercept: {intercept}')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 添加常数项，对应线性回归模型的截距\n",
    "encoded_features_train_with_intercept = sm.add_constant(encoded_features_train)\n",
    "\n",
    "# 使用statsmodels的OLS函数训练线性回归模型\n",
    "model = sm.OLS(target_train, encoded_features_train_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# 打印每个特征的回归系数、标准误差和p值\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9ed7cf-a015-4f68-8ccd-c0e316b4d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Return  Volatility     Beta1     Beta2      Cor1      Cor2  \\\n",
      "0      0.200627    0.583365  0.716008  0.726709  0.184198  0.188873   \n",
      "1      0.200501    0.383995  0.913849  0.923003  0.409555  0.416662   \n",
      "2      0.200431    0.423533  0.806881  0.831737  0.278495  0.292493   \n",
      "3      0.200397    0.518040  0.943050  0.929599  0.254375  0.254799   \n",
      "4      0.200378    0.550201  1.066480  1.068627  0.284123  0.290532   \n",
      "...         ...         ...       ...       ...       ...       ...   \n",
      "78552  0.000139    0.555577  0.944254  0.961975  0.247322  0.256579   \n",
      "78553  0.000138    0.471594  1.882598  1.846909  0.568810  0.567869   \n",
      "78554  0.000133    0.649889  1.180658  1.213428  0.317196  0.328823   \n",
      "78555  0.000115    0.632866  1.286845  1.315512  0.349781  0.361143   \n",
      "78556  0.000100    0.645599  1.472637  1.493594  0.335231  0.346022   \n",
      "\n",
      "       NonSysRisk1  NonSysRisk2      Rsq1      Rsq2  ...  \\\n",
      "0         0.012349     0.012984  0.033929  0.035673  ...   \n",
      "1         0.025676     0.026575  0.167735  0.173607  ...   \n",
      "2         0.014550     0.016050  0.077559  0.085552  ...   \n",
      "3         0.018665     0.018727  0.064707  0.064923  ...   \n",
      "4         0.025388     0.026546  0.080726  0.084409  ...   \n",
      "...            ...          ...       ...       ...  ...   \n",
      "78552     0.019359     0.020836  0.061168  0.065833  ...   \n",
      "78553     0.074968     0.074720  0.323544  0.322475  ...   \n",
      "78554     0.042696     0.045884  0.100613  0.108125  ...   \n",
      "78555     0.049497     0.052765  0.122346  0.130424  ...   \n",
      "78556     0.047210     0.050299  0.112380  0.119731  ...   \n",
      "\n",
      "       NetProfitGrowth EquityGrowth  NetProfitGrowth RevenueGrowth  \\\n",
      "0                          0.012195                       0.053977   \n",
      "1                          0.056986                       0.020900   \n",
      "2                          0.023526                      -0.001153   \n",
      "3                         -0.004921                      -0.017542   \n",
      "4                          0.092004                       0.275604   \n",
      "...                             ...                            ...   \n",
      "78552                      1.659731                       2.828017   \n",
      "78553                      0.065739                       0.091202   \n",
      "78554                    104.639525                     967.833959   \n",
      "78555                      0.371093                       0.293152   \n",
      "78556                      0.089185                       0.039416   \n",
      "\n",
      "       NetProfitGrowth OperatingNCFGrowth  NetProfitGrowth OGS  \\\n",
      "0                               -0.001129             0.030168   \n",
      "1                               -0.086616             0.042656   \n",
      "2                               -0.009707             0.007212   \n",
      "3                               -0.095343            -0.026953   \n",
      "4                                1.517847             0.942245   \n",
      "...                                   ...                  ...   \n",
      "78552                            8.903582             4.885746   \n",
      "78553                            0.160265             0.106475   \n",
      "78554                            8.745639          1323.269817   \n",
      "78555                           -1.040409             0.011173   \n",
      "78556                            0.068392             0.070715   \n",
      "\n",
      "       EquityGrowth RevenueGrowth  EquityGrowth OperatingNCFGrowth  \\\n",
      "0                        0.011834                        -0.000247   \n",
      "1                        0.006640                        -0.027520   \n",
      "2                       -0.001676                        -0.014111   \n",
      "3                        0.008637                         0.046943   \n",
      "4                        0.013462                         0.074142   \n",
      "...                           ...                              ...   \n",
      "78552                    0.763005                         2.402207   \n",
      "78553                    0.055161                         0.096932   \n",
      "78554                   24.044883                         0.217277   \n",
      "78555                    0.258489                        -0.917388   \n",
      "78556                    0.040939                         0.071035   \n",
      "\n",
      "       EquityGrowth OGS  RevenueGrowth OperatingNCFGrowth  RevenueGrowth OGS  \\\n",
      "0              0.006614                         -0.001095           0.029273   \n",
      "1              0.013553                         -0.010093           0.004971   \n",
      "2              0.010485                          0.000691          -0.000514   \n",
      "3              0.013270                          0.167349           0.047309   \n",
      "4              0.046026                          0.222097           0.137873   \n",
      "...                 ...                               ...                ...   \n",
      "78552          1.318186                          4.093123           2.246058   \n",
      "78553          0.064398                          0.134478           0.089342   \n",
      "78554         32.875337                          2.009641         304.071218   \n",
      "78555          0.009852                         -0.724709           0.007783   \n",
      "78556          0.073448                          0.031394           0.032461   \n",
      "\n",
      "       OperatingNCFGrowth OGS  \n",
      "0                   -0.000612  \n",
      "1                   -0.020600  \n",
      "2                   -0.004326  \n",
      "3                    0.257121  \n",
      "4                    0.759313  \n",
      "...                       ...  \n",
      "78552                7.071373  \n",
      "78553                0.156997  \n",
      "78554                2.747679  \n",
      "78555               -0.027621  \n",
      "78556                0.056324  \n",
      "\n",
      "[78557 rows x 630 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 数据整理\n",
    "data = pd.read_csv('/Users/xiaoquanliu/Desktop/Book_DataCode/第七章/DL_Data1.csv')\n",
    "data1=data.dropna()\n",
    "\n",
    "features = data1.iloc[:, :-1] \n",
    "target = data1.iloc[:, -1]\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "# 创建一个 PolynomialFeatures 对象，设置 degree=2 来生成两两交互的特征\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "# 使用 PolynomialFeatures 对象转换你的数据\n",
    "features_poly = poly.fit_transform(features)\n",
    "\n",
    "# 将交互特征转换为 DataFrame，并添加列名\n",
    "features_poly_df = pd.DataFrame(features_poly, columns=poly.get_feature_names_out(features.columns))\n",
    "\n",
    "# 打印结果\n",
    "print(features_poly_df)\n",
    "\n",
    "# features_poly_df 包含了原始特征和它们的两两交互项\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_poly_df)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "input_dim = features_train.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "321c88bc-3140-4cc8-ae10-177082d703d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/123 [==============================] - 1s 3ms/step - loss: 0.1709 - val_loss: 0.1691\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1669 - val_loss: 0.1641\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1605 - val_loss: 0.1561\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 0.1451\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1388 - val_loss: 0.1319\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 0.1173\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1099 - val_loss: 0.1023\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.0877\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 0.0742\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0682 - val_loss: 0.0622\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.0518\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.0432\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.0361\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0332 - val_loss: 0.0304\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0280 - val_loss: 0.0257\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0237 - val_loss: 0.0219\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0188\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0174 - val_loss: 0.0162\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0151 - val_loss: 0.0142\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0133 - val_loss: 0.0126\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0097 - val_loss: 0.0093\n",
      "Epoch 24/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0089 - val_loss: 0.0086\n",
      "Epoch 25/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 26/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 27/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 28/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 29/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 30/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 31/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 32/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 33/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 34/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 35/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 36/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 37/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 38/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 39/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 40/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 41/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 42/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 43/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 44/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 45/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 46/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 47/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 48/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 49/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 50/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 51/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 52/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 53/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 54/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 55/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 56/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 57/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 58/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 59/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 60/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 61/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 62/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 63/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 64/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 65/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 66/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 67/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 68/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 69/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 70/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 71/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 72/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 73/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 74/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 75/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 76/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 77/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 78/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 79/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 80/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 81/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 82/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 83/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 84/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 85/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 86/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 87/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 88/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 89/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 90/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 91/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 92/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 93/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 94/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 95/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 96/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 97/100\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 98/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 99/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 100/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "1964/1964 [==============================] - 1s 328us/step\n",
      "491/491 [==============================] - 0s 365us/step\n",
      "1964/1964 [==============================] - 1s 333us/step\n",
      "Mean Squared Error on test set:  0.6571058266480831\n",
      "Coefficient for encoded feature 1: 2.924063205718994\n",
      "Coefficient for encoded feature 2: 6.812810897827148e-05\n",
      "Coefficient for encoded feature 3: 12.87165355682373\n",
      "Coefficient for encoded feature 4: 3.6975388526916504\n",
      "Coefficient for encoded feature 5: -7.844210624694824\n",
      "Coefficient for encoded feature 6: 3.291097640991211\n",
      "Coefficient for encoded feature 7: 2.4378747940063477\n",
      "Coefficient for encoded feature 8: 6.627534866333008\n",
      "Coefficient for encoded feature 9: 1.7903335094451904\n",
      "Coefficient for encoded feature 10: 1.987451195716858\n",
      "Coefficient for encoded feature 11: 1.0672154426574707\n",
      "Coefficient for encoded feature 12: 2.9587082862854004\n",
      "Coefficient for encoded feature 13: -10.337772369384766\n",
      "Coefficient for encoded feature 14: 2.8195810317993164\n",
      "Coefficient for encoded feature 15: -1.2040138244628906e-05\n",
      "Coefficient for encoded feature 16: 5.361598014831543\n",
      "Coefficient for encoded feature 17: 6.104448318481445\n",
      "Coefficient for encoded feature 18: 5.245208740234375e-06\n",
      "Coefficient for encoded feature 19: -2.960145950317383\n",
      "Coefficient for encoded feature 20: -2.86637544631958\n",
      "Coefficient for encoded feature 21: -14.012548446655273\n",
      "Coefficient for encoded feature 22: 8.597481727600098\n",
      "Coefficient for encoded feature 23: -0.6690073013305664\n",
      "Coefficient for encoded feature 24: -6.958977699279785\n",
      "Coefficient for encoded feature 25: 9.534939765930176\n",
      "Coefficient for encoded feature 26: -10.62984561920166\n",
      "Coefficient for encoded feature 27: 6.564793586730957\n",
      "Coefficient for encoded feature 28: -7.120159149169922\n",
      "Coefficient for encoded feature 29: 7.3488078117370605\n",
      "Coefficient for encoded feature 30: -2.7757296562194824\n",
      "Coefficient for encoded feature 31: -3.038836717605591\n",
      "Coefficient for encoded feature 32: 6.00206184387207\n",
      "Coefficient for encoded feature 33: 4.045269012451172\n",
      "Coefficient for encoded feature 34: 10.895901679992676\n",
      "Coefficient for encoded feature 35: -2.466691017150879\n",
      "Coefficient for encoded feature 36: 12.672516822814941\n",
      "Coefficient for encoded feature 37: -4.286381721496582\n",
      "Coefficient for encoded feature 38: -3.9988503456115723\n",
      "Coefficient for encoded feature 39: -2.0272130966186523\n",
      "Coefficient for encoded feature 40: -9.65007209777832\n",
      "Coefficient for encoded feature 41: -1.0636115074157715\n",
      "Coefficient for encoded feature 42: 0.9621992111206055\n",
      "Coefficient for encoded feature 43: -1.539475440979004\n",
      "Coefficient for encoded feature 44: 1.3218870162963867\n",
      "Coefficient for encoded feature 45: -6.32989501953125\n",
      "Coefficient for encoded feature 46: 3.500992774963379\n",
      "Coefficient for encoded feature 47: -1.7743730545043945\n",
      "Coefficient for encoded feature 48: 8.70352840423584\n",
      "Coefficient for encoded feature 49: 5.229701042175293\n",
      "Coefficient for encoded feature 50: 2.0742416381835938e-05\n",
      "Coefficient for encoded feature 51: 2.1457672119140625e-05\n",
      "Coefficient for encoded feature 52: 6.154898643493652\n",
      "Coefficient for encoded feature 53: -19.251781463623047\n",
      "Coefficient for encoded feature 54: -1.696510672569275\n",
      "Coefficient for encoded feature 55: -2.0731427669525146\n",
      "Coefficient for encoded feature 56: -4.9319329261779785\n",
      "Coefficient for encoded feature 57: 3.498018503189087\n",
      "Coefficient for encoded feature 58: 8.571606636047363\n",
      "Coefficient for encoded feature 59: 0.0\n",
      "Coefficient for encoded feature 60: -14.273679733276367\n",
      "Intercept: -50.33275604248047\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    VCG   R-squared:                       0.921\n",
      "Model:                            OLS   Adj. R-squared:                  0.921\n",
      "Method:                 Least Squares   F-statistic:                 1.350e+04\n",
      "Date:                Mon, 20 Nov 2023   Prob (F-statistic):               0.00\n",
      "Time:                        18:18:58   Log-Likelihood:                -75493.\n",
      "No. Observations:               62845   AIC:                         1.511e+05\n",
      "Df Residuals:                   62790   BIC:                         1.516e+05\n",
      "Df Model:                          54                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -50.3330      0.190   -265.382      0.000     -50.705     -49.961\n",
      "x1             2.9241      0.113     25.901      0.000       2.703       3.145\n",
      "x2         -1.795e-12   1.85e-13     -9.683      0.000   -2.16e-12   -1.43e-12\n",
      "x3            12.8717      0.169     76.336      0.000      12.541      13.202\n",
      "x4             3.6976      0.114     32.563      0.000       3.475       3.920\n",
      "x5            -7.8445      3.956     -1.983      0.047     -15.599      -0.090\n",
      "x6             3.2911      0.152     21.592      0.000       2.992       3.590\n",
      "x7             2.4379      0.159     15.364      0.000       2.127       2.749\n",
      "x8             6.6276      0.128     51.615      0.000       6.376       6.879\n",
      "x9             1.7903      0.154     11.652      0.000       1.489       2.091\n",
      "x10            1.9875      0.046     43.515      0.000       1.898       2.077\n",
      "x11            1.0672      0.153      6.956      0.000       0.766       1.368\n",
      "x12            2.9587      0.124     23.903      0.000       2.716       3.201\n",
      "x13          -10.3378      0.176    -58.758      0.000     -10.683      -9.993\n",
      "x14            2.8196      0.150     18.736      0.000       2.525       3.115\n",
      "x15        -6.162e-14   1.18e-13     -0.524      0.600   -2.92e-13    1.69e-13\n",
      "x16            5.3616      0.153     35.065      0.000       5.062       5.661\n",
      "x17            6.1044      0.139     43.976      0.000       5.832       6.377\n",
      "x18         6.108e-14   1.57e-13      0.388      0.698   -2.47e-13     3.7e-13\n",
      "x19           -2.9601      0.173    -17.157      0.000      -3.298      -2.622\n",
      "x20           -2.8664      0.128    -22.418      0.000      -3.117      -2.616\n",
      "x21          -14.0125      0.151    -93.027      0.000     -14.308     -13.717\n",
      "x22            8.5976      0.135     63.665      0.000       8.333       8.862\n",
      "x23           -0.6690      0.148     -4.524      0.000      -0.959      -0.379\n",
      "x24           -6.9590      0.139    -49.976      0.000      -7.232      -6.686\n",
      "x25            9.4753     16.836      0.563      0.574     -23.524      42.475\n",
      "x26          -10.6299      0.119    -89.192      0.000     -10.863     -10.396\n",
      "x27            6.5649      0.169     38.924      0.000       6.234       6.895\n",
      "x28           -7.1202      0.167    -42.523      0.000      -7.448      -6.792\n",
      "x29            7.3486      0.642     11.438      0.000       6.089       8.608\n",
      "x30           -2.7757      0.133    -20.930      0.000      -3.036      -2.516\n",
      "x31           -3.0389      0.144    -21.136      0.000      -3.321      -2.757\n",
      "x32            6.0021      0.123     48.931      0.000       5.762       6.243\n",
      "x33            4.0452      0.183     22.129      0.000       3.687       4.404\n",
      "x34           10.8959      0.159     68.598      0.000      10.585      11.207\n",
      "x35           -2.4667      0.102    -24.090      0.000      -2.667      -2.266\n",
      "x36           12.6725      0.145     87.446      0.000      12.388      12.957\n",
      "x37           -4.2864      0.167    -25.669      0.000      -4.614      -3.959\n",
      "x38           -3.9988      0.179    -22.314      0.000      -4.350      -3.648\n",
      "x39           -2.0272      0.129    -15.696      0.000      -2.280      -1.774\n",
      "x40           -9.5892     16.166     -0.593      0.553     -41.275      22.097\n",
      "x41           -1.0636      0.113     -9.384      0.000      -1.286      -0.841\n",
      "x42            0.9622      0.174      5.537      0.000       0.622       1.303\n",
      "x43           -1.5395      0.122    -12.632      0.000      -1.778      -1.301\n",
      "x44            1.3219      0.170      7.755      0.000       0.988       1.656\n",
      "x45           -6.3298      0.165    -38.431      0.000      -6.653      -6.007\n",
      "x46            3.5010      0.129     27.059      0.000       3.247       3.755\n",
      "x47           -1.7744      0.189     -9.389      0.000      -2.145      -1.404\n",
      "x48            8.7035      0.142     61.432      0.000       8.426       8.981\n",
      "x49            5.2297      0.145     36.105      0.000       4.946       5.514\n",
      "x50        -1.516e-15   5.47e-14     -0.028      0.978   -1.09e-13    1.06e-13\n",
      "x51        -8.375e-14   1.05e-13     -0.800      0.424   -2.89e-13    1.21e-13\n",
      "x52            6.1549      0.124     49.527      0.000       5.911       6.398\n",
      "x53          -19.2518      0.120   -160.025      0.000     -19.488     -19.016\n",
      "x54           -1.6965      0.169    -10.028      0.000      -2.028      -1.365\n",
      "x55           -2.0731      0.126    -16.412      0.000      -2.321      -1.826\n",
      "x56           -4.9319      0.141    -34.868      0.000      -5.209      -4.655\n",
      "x57            3.4980      0.127     27.518      0.000       3.249       3.747\n",
      "x58            8.5716      0.158     54.285      0.000       8.262       8.881\n",
      "x59                 0          0        nan        nan           0           0\n",
      "x60          -14.2737      0.150    -95.308      0.000     -14.567     -13.980\n",
      "==============================================================================\n",
      "Omnibus:                    52463.803   Durbin-Watson:                   2.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         28119682.924\n",
      "Skew:                           2.947   Prob(JB):                         0.00\n",
      "Kurtosis:                     106.460   Cond. No.                     1.00e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.76e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#设置隐藏因子的数量\n",
    "encoding_dim = 60\n",
    "hidden_dim=315\n",
    "num_hidden_layer=10\n",
    "\n",
    "\n",
    "# 添加L1正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(0.01 ))\n",
    "\n",
    "# 添加L2正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "decoding_dim=256\n",
    "input_layer = Input(shape=(630,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoder_layer)\n",
    "\n",
    "# 自编码器模型\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))  # 解码层\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "# 训练自编码器模型，使用训练集的特征进行训练和验证\n",
    "autoencoder.fit(features_train, features_train, epochs=100,batch_size=512, shuffle=True, validation_data=(features_test, features_test))\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "encoded_features_test = encoder.predict(features_test)\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "\n",
    "# 转换为Pandas DataFrame\n",
    "encoded_features_train_df = pd.DataFrame(encoded_features_train)\n",
    "\n",
    "\n",
    "# 使用降维后的特征训练预测模型，这里我们使用一个简单的线性回归模型作为例子\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 对测试集进行预测并评估模型性能\n",
    "predictions = regressor.predict(encoded_features_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "print('Mean Squared Error on test set: ', mse)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 使用降维后的特征训练线性回归模型\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 获取线性回归模型的参数\n",
    "coefficients = regressor.coef_  # 回归系数，对应每个特征的权重\n",
    "intercept = regressor.intercept_  # 截距项\n",
    "\n",
    "# 打印每个特征的回归系数\n",
    "for i, coef in enumerate(coefficients):\n",
    "    print(f'Coefficient for encoded feature {i+1}: {coef}')\n",
    "\n",
    "# 打印截距项\n",
    "print(f'Intercept: {intercept}')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 添加常数项，对应线性回归模型的截距\n",
    "encoded_features_train_with_intercept = sm.add_constant(encoded_features_train)\n",
    "\n",
    "# 使用statsmodels的OLS函数训练线性回归模型\n",
    "model = sm.OLS(target_train, encoded_features_train_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# 打印每个特征的回归系数、标准误差和p值\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0248056-4973-4863-895a-d1ff88b8d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.1633 - val_loss: 0.1619\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1600 - val_loss: 0.1577\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1550 - val_loss: 0.1520\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1485 - val_loss: 0.1446\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.1358\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1310 - val_loss: 0.1259\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1206 - val_loss: 0.1152\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1095 - val_loss: 0.1038\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0982 - val_loss: 0.0926\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0874 - val_loss: 0.0821\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0772 - val_loss: 0.0723\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.0634\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0555\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0484\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.0423\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0370\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0324\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0284\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0222\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0197\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.0176\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0159\n",
      "Epoch 24/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0143\n",
      "Epoch 25/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0130\n",
      "Epoch 26/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0119\n",
      "Epoch 27/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 28/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 29/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 30/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 31/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 32/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 33/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 34/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 35/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 36/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 37/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 38/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 39/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 40/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 41/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 42/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 43/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 44/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 45/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 46/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 47/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 48/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 49/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 50/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 51/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 52/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 53/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 54/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 55/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 56/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 57/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 58/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 59/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 60/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 61/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 62/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 63/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 64/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 65/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 66/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 67/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 68/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 69/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 70/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 71/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 72/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 73/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 74/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 75/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 76/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 77/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 78/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 79/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 80/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 81/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 82/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 83/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 84/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 85/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 86/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 87/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 88/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 89/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 90/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 91/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 92/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 93/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 94/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 95/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 96/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 97/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 98/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 99/100\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 100/100\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "1964/1964 [==============================] - 1s 303us/step\n",
      "491/491 [==============================] - 0s 332us/step\n",
      "1964/1964 [==============================] - 1s 304us/step\n",
      "Mean Squared Error on test set:  0.0003976117776417458\n",
      "Coefficient for encoded feature 1: -0.00795748457312584\n",
      "Coefficient for encoded feature 2: -4.800967872142792e-07\n",
      "Coefficient for encoded feature 3: -0.004537142813205719\n",
      "Coefficient for encoded feature 4: -0.007712570950388908\n",
      "Coefficient for encoded feature 5: 1.6763806343078613e-07\n",
      "Coefficient for encoded feature 6: 0.023343609645962715\n",
      "Coefficient for encoded feature 7: 0.03574550524353981\n",
      "Coefficient for encoded feature 8: -0.008243638090789318\n",
      "Coefficient for encoded feature 9: -0.04750257357954979\n",
      "Coefficient for encoded feature 10: 2.60770320892334e-08\n",
      "Coefficient for encoded feature 11: 0.06741402298212051\n",
      "Coefficient for encoded feature 12: 0.03822584077715874\n",
      "Coefficient for encoded feature 13: -0.01095853466540575\n",
      "Coefficient for encoded feature 14: -0.00447995588183403\n",
      "Coefficient for encoded feature 15: 0.018920859321951866\n",
      "Coefficient for encoded feature 16: 0.04421055316925049\n",
      "Coefficient for encoded feature 17: -0.08881940692663193\n",
      "Coefficient for encoded feature 18: 0.029090750962495804\n",
      "Coefficient for encoded feature 19: -0.009891818277537823\n",
      "Coefficient for encoded feature 20: -0.0139826200902462\n",
      "Coefficient for encoded feature 21: -0.034072939306497574\n",
      "Coefficient for encoded feature 22: 0.025652507320046425\n",
      "Coefficient for encoded feature 23: 0.01575940102338791\n",
      "Coefficient for encoded feature 24: -0.021050795912742615\n",
      "Coefficient for encoded feature 25: 0.004187123849987984\n",
      "Coefficient for encoded feature 26: -0.021393178030848503\n",
      "Coefficient for encoded feature 27: -0.012578831985592842\n",
      "Coefficient for encoded feature 28: 0.0\n",
      "Coefficient for encoded feature 29: 0.01271895319223404\n",
      "Coefficient for encoded feature 30: 0.01269029825925827\n",
      "Intercept: -0.031263548880815506\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Return   R-squared:                       0.153\n",
      "Model:                            OLS   Adj. R-squared:                  0.153\n",
      "Method:                 Least Squares   F-statistic:                     437.3\n",
      "Date:                Sun, 15 Sep 2024   Prob (F-statistic):               0.00\n",
      "Time:                        21:50:21   Log-Likelihood:             1.5525e+05\n",
      "No. Observations:               62845   AIC:                        -3.104e+05\n",
      "Df Residuals:                   62818   BIC:                        -3.102e+05\n",
      "Df Model:                          26                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0313      0.003    -10.030      0.000      -0.037      -0.025\n",
      "x1            -0.0080      0.002     -3.355      0.001      -0.013      -0.003\n",
      "x2         -1.353e-15    4.3e-16     -3.145      0.002    -2.2e-15    -5.1e-16\n",
      "x3            -0.0045      0.004     -1.293      0.196      -0.011       0.002\n",
      "x4            -0.0077      0.003     -2.507      0.012      -0.014      -0.002\n",
      "x5         -1.846e-16   6.39e-17     -2.888      0.004    -3.1e-16   -5.93e-17\n",
      "x6             0.0233      0.002     10.054      0.000       0.019       0.028\n",
      "x7             0.0357      0.003     11.218      0.000       0.030       0.042\n",
      "x8            -0.0082      0.003     -2.753      0.006      -0.014      -0.002\n",
      "x9            -0.0475      0.002    -21.182      0.000      -0.052      -0.043\n",
      "x10         3.278e-17   9.73e-18      3.369      0.001    1.37e-17    5.18e-17\n",
      "x11            0.0674      0.003     19.353      0.000       0.061       0.074\n",
      "x12            0.0382      0.003     15.229      0.000       0.033       0.043\n",
      "x13           -0.0110      0.003     -4.200      0.000      -0.016      -0.006\n",
      "x14           -0.0045      0.002     -2.623      0.009      -0.008      -0.001\n",
      "x15            0.0189      0.003      6.939      0.000       0.014       0.024\n",
      "x16            0.0442      0.003     14.357      0.000       0.038       0.050\n",
      "x17           -0.0888      0.003    -32.728      0.000      -0.094      -0.084\n",
      "x18            0.0291      0.003      9.294      0.000       0.023       0.035\n",
      "x19           -0.0099      0.003     -3.842      0.000      -0.015      -0.005\n",
      "x20           -0.0140      0.003     -5.493      0.000      -0.019      -0.009\n",
      "x21           -0.0341      0.003    -12.821      0.000      -0.039      -0.029\n",
      "x22            0.0257      0.003      8.702      0.000       0.020       0.031\n",
      "x23            0.0158      0.002      7.580      0.000       0.012       0.020\n",
      "x24           -0.0211      0.003     -7.901      0.000      -0.026      -0.016\n",
      "x25            0.0042      0.002      1.779      0.075      -0.000       0.009\n",
      "x26           -0.0214      0.003     -7.610      0.000      -0.027      -0.016\n",
      "x27           -0.0126      0.003     -4.183      0.000      -0.018      -0.007\n",
      "x28                 0          0        nan        nan           0           0\n",
      "x29            0.0127      0.002      6.807      0.000       0.009       0.016\n",
      "x30            0.0127      0.002      5.511      0.000       0.008       0.017\n",
      "==============================================================================\n",
      "Omnibus:                    25368.258   Durbin-Watson:                   1.993\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           135429.622\n",
      "Skew:                           1.892   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.115   Cond. No.                     1.00e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.06e-25. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#设置隐藏因子的数量\n",
    "encoding_dim = 30\n",
    "hidden_dim=315\n",
    "num_hidden_layer=20\n",
    "\n",
    "\n",
    "# 添加L1正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(0.01 ))\n",
    "\n",
    "# 添加L2正则化\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "decoding_dim=256\n",
    "input_layer = Input(shape=(630,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoder_layer)\n",
    "\n",
    "# 自编码器模型\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))  # 解码层\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "# 训练自编码器模型，使用训练集的特征进行训练和验证\n",
    "autoencoder.fit(features_train, features_train, epochs=100,batch_size=512, shuffle=True, validation_data=(features_test, features_test))\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "encoded_features_test = encoder.predict(features_test)\n",
    "\n",
    "# 使用编码器对特征进行降维\n",
    "encoded_features_train = encoder.predict(features_train)\n",
    "\n",
    "# 转换为Pandas DataFrame\n",
    "encoded_features_train_df = pd.DataFrame(encoded_features_train)\n",
    "\n",
    "\n",
    "# 使用降维后的特征训练预测模型，这里我们使用一个简单的线性回归模型作为例子\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 对测试集进行预测并评估模型性能\n",
    "predictions = regressor.predict(encoded_features_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "print('Mean Squared Error on test set: ', mse)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 使用降维后的特征训练线性回归模型\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(encoded_features_train, target_train)\n",
    "\n",
    "# 获取线性回归模型的参数\n",
    "coefficients = regressor.coef_  # 回归系数，对应每个特征的权重\n",
    "intercept = regressor.intercept_  # 截距项\n",
    "\n",
    "# 打印每个特征的回归系数\n",
    "for i, coef in enumerate(coefficients):\n",
    "    print(f'Coefficient for encoded feature {i+1}: {coef}')\n",
    "\n",
    "# 打印截距项\n",
    "print(f'Intercept: {intercept}')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 添加常数项，对应线性回归模型的截距\n",
    "encoded_features_train_with_intercept = sm.add_constant(encoded_features_train)\n",
    "\n",
    "# 使用statsmodels的OLS函数训练线性回归模型\n",
    "model = sm.OLS(target_train, encoded_features_train_with_intercept)\n",
    "results = model.fit()\n",
    "\n",
    "# 打印每个特征的回归系数、标准误差和p值\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d9e7b-86d5-4375-aff2-c5e6817e3ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
